# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n762ZSUq6cjYCIvLW_XvC4X8FY7WfZuH
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("vagifa/ethereum-frauddetection-dataset")

print("Path to dataset files:", path)

# Colab cell 0: install deps and unzip dataset

# If the kagglehub download returned a path string saved to a var, you might have it in Python.
# But we'll also try to find the zip or csv in current dir:
import os, glob
print("Files in working dir:")
print(os.listdir('.'))

# try unzip any zip files created by kagglehub
zips = glob.glob("*.zip")
for z in zips:
    print("Unzipping", z)
    !unzip -o "{z}" -d dataset_unzipped

# list possible CSVs
csvs = glob.glob("**/*.csv", recursive=True)
print("Found CSVs (first 20):", csvs[:20])

import kagglehub
path = kagglehub.dataset_download("vagifa/ethereum-frauddetection-dataset")
print("Dataset path:", path)
!ls -lh $path

from tqdm import tqdm
import pandas as pd, numpy as np

DATA_CSV = "/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv"

df = pd.read_csv(DATA_CSV, low_memory=False)
print("Columns:", list(df.columns)[:20])
print("Total rows:", len(df))
print("Fraud count:", int(df['FLAG'].sum()), "/", len(df))

# ---- Heuristic rules ----
def rule_phishing(row, thr_unique_received=30, thr_avg_recv_min=60):
    return (row.get('Unique Received From Addresses',0) >= thr_unique_received) and (
        (row.get('ERC20 avg val sent',0) < 1e4) or
        (row.get('Avg min between received tnx',1e9) > thr_avg_recv_min)
    )

def rule_ponzi(row):
    return (row.get('Received Tnx',0) >= 200) and \
           (row.get('Sent tnx',0) >= 50) and \
           (row.get('Time Diff between first and last (Mins)',0) > 24*60)

def rule_wash(row):
    total = row.get('Sent tnx',0) + row.get('Received Tnx',0)
    uniq = row.get('ERC20 uniq sent token name', 0)
    return (total > 0) and (uniq <= 3) and (total >= 100)

def rule_rug(row):
    return (row.get('Number of Created Contracts',0) >= 1) and \
           (row.get('ERC20 max val sent contract',0) > 1e5) and \
           (row.get('Time Diff between first and last (Mins)',0) < 24*60)

def rule_mixer(row):
    return (row.get('Avg min between sent tnx',1e9) < 5) and \
           (row.get('Sent tnx',0) >= 50)

# ---- Apply rules only to fraud rows ----
fraud = df[df['FLAG']==1].copy()
rows = []
for _, r in tqdm(fraud.iterrows(), total=len(fraud)):
    labels = []
    if rule_phishing(r): labels.append('phishing')
    if rule_ponzi(r): labels.append('ponzi')
    if rule_wash(r): labels.append('wash')
    if rule_rug(r): labels.append('rug_pull')
    if rule_mixer(r): labels.append('mixer')
    if not labels: labels.append('unknown')
    rows.append({'address': r['Address'], 'candidate_labels': ",".join(labels)})

out = pd.DataFrame(rows)
print("\nLabel distribution:")
print(out['candidate_labels'].value_counts())

print("\nSample rows:")
print(out.head(10))

out.to_csv("labels_candidates.csv", index=False)
print("\nSaved labels_candidates.csv")

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import pandas as pd

DATA_CSV = "/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv"

# load original + candidates
orig = pd.read_csv(DATA_CSV, low_memory=False)
cand = pd.read_csv("labels_candidates.csv")

# merge back numeric features for clustering
merged = cand.merge(orig, left_on="address", right_on="Address", how="left")

numcols = [c for c in merged.columns if merged[c].dtype.kind in "if" and c not in ["FLAG"]]
X = merged[numcols].fillna(0).values
Xs = StandardScaler().fit_transform(X)
pca = PCA(n_components=min(10, Xs.shape[1])).fit_transform(Xs)

# cluster
k = 6  # we can adjust later
kmeans = KMeans(n_clusters=k, random_state=42)
merged["cluster_id"] = kmeans.fit_predict(pca)

print("Cluster sizes:\n", merged["cluster_id"].value_counts())
print("\nSample rows:")
print(merged[["address","candidate_labels","cluster_id"]].head(20))

merged.to_csv("labels_candidates_with_cluster.csv", index=False)
print("\nSaved labels_candidates_with_cluster.csv")

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import hdbscan

DATA_CSV = "/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv"

orig = pd.read_csv(DATA_CSV, low_memory=False)
cand = pd.read_csv("labels_candidates.csv")
merged = cand.merge(orig, left_on="address", right_on="Address", how="left")

# pick numeric cols
numcols = [c for c in merged.columns if merged[c].dtype.kind in "if" and c not in ["FLAG"]]
X = merged[numcols].fillna(0).values
Xs = StandardScaler().fit_transform(X)
pca = PCA(n_components=min(10, Xs.shape[1])).fit_transform(Xs)

# HDBSCAN clustering
clusterer = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=10)
merged["cluster_id"] = clusterer.fit_predict(pca)

print("Cluster sizes:\n", merged["cluster_id"].value_counts().sort_index())
print("\nSample rows:")
print(merged[["address","candidate_labels","cluster_id"]].head(20))

merged.to_csv("labels_candidates_with_cluster.csv", index=False)
print("\nSaved labels_candidates_with_cluster.csv")

import pandas as pd

DATA_CSV = "/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv"
orig = pd.read_csv(DATA_CSV, low_memory=False)
cand_cluster = pd.read_csv("labels_candidates_with_cluster.csv")

merged = cand_cluster.merge(orig, left_on="address", right_on="Address", how="left")

# Pick useful columns for manual labeling
cols = [
    "address","candidate_labels","cluster_id","Sent tnx","Received Tnx",
    "Unique Received From Addresses","Unique Sent To Addresses",
    "Number of Created Contracts","Avg min between sent tnx",
    "Avg min between received tnx","Time Diff between first and last (Mins)",
    "ERC20 uniq sent token name","ERC20 uniq rec token name",
    "ERC20 max val sent","ERC20 max val sent contract"
]
present = [c for c in cols if c in merged.columns]

sheet = merged[present].copy()
sheet.to_excel("labels_for_annotation.xlsx", index=False)
print("Wrote labels_for_annotation.xlsx → download & upload to Google Sheets for labeling.")

import pandas as pd

# Load merged file
merged = pd.read_csv("labels_candidates_with_cluster.csv")
orig = pd.read_csv("/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv", low_memory=False)
merged = merged.merge(orig, left_on="address", right_on="Address", how="left")

# Pick numeric features dynamically
numcols = [c for c in merged.columns if merged[c].dtype.kind in "if"]
print("Numeric columns (first 20):", numcols[:20])

# Compute average per cluster
cluster_summary = merged.groupby("cluster_id")[numcols].mean().round(2)
print("\nCluster summary (mean values):")
print(cluster_summary.head(10))

import requests, pandas as pd, time

API_KEY = "LK2d4I9WU_eYty56pr1hq"  # put your Alchemy key here
BASE_URL = f"https://eth-mainnet.g.alchemy.com/v2/{API_KEY}"

# Pick sample addresses from clusters 0, 2, and -1
merged = pd.read_csv("labels_candidates_with_cluster.csv")
sample_addrs = (
    merged[merged['cluster_id'].isin([0,2,-1])]
    .sample(30, random_state=42)['address']
    .tolist()
)

rows = []
for addr in sample_addrs:
    url = BASE_URL
    payload = {
        "jsonrpc": "2.0",
        "id": 1,
        "method": "alchemy_getAssetTransfers",
        "params": [{
            "fromBlock": "0x0",
            "toBlock": "latest",
            "toAddress": addr,
            "category": ["external","erc20","erc721"],
            "maxCount": "0x14"   # ~20 txs
        }]
    }
    try:
        r = requests.post(url, json=payload, timeout=15)
        data = r.json().get("result", {}).get("transfers", [])
        rows.append({
            "address": addr,
            "tx_count": len(data),
            "first_tx": data[0]["blockNum"] if data else None,
            "last_tx": data[-1]["blockNum"] if data else None,
            "unique_senders": len(set([d["from"] for d in data])),
            "unique_receivers": len(set([d["to"] for d in data if "to" in d]))
        })
    except Exception as e:
        rows.append({"address": addr, "error": str(e)})
    time.sleep(0.4)

pd.DataFrame(rows).to_csv("alchemy_summary.csv", index=False)
print("Saved alchemy_summary.csv")

# Colab: 1) Alchemy enrichment (edit API_KEY)
import requests, pandas as pd, time, json
from random import Random

API_KEY = "LK2d4I9WU_eYty56pr1hq"   # <<< put your key here
BASE_URL = f"https://eth-mainnet.g.alchemy.com/v2/{API_KEY}"

# load addresses from clustered candidates
merged = pd.read_csv("labels_candidates_with_cluster.csv")
# pick 30 addresses balanced across cluster 0,2,-1 if available
rng = Random(42)
candidates = merged[merged['cluster_id'].isin([0,2,-1])]
sample_size = min(30, len(candidates))
sample_addrs = candidates['address'].tolist()
if len(sample_addrs) > sample_size:
    sample_addrs = rng.sample(sample_addrs, sample_size)

rows=[]
for addr in sample_addrs:
    try:
        payload = {
            "jsonrpc":"2.0","id":1,"method":"alchemy_getAssetTransfers",
            "params":[{"fromBlock":"0x0","toBlock":"latest","toAddress":addr,
                       "category":["external","erc20","erc721"], "maxCount":"0x64"}]
        }
        r = requests.post(BASE_URL, json=payload, timeout=20)
        res = r.json()
        transfers = res.get("result", {}).get("transfers", []) if isinstance(res, dict) else []
        # quick summary
        tx_count = len(transfers)
        senders = set([t.get('from') for t in transfers if t.get('from')])
        receivers = set([t.get('to') for t in transfers if t.get('to')])
        interacts_with_contract = any(t.get('rawContract') or (t.get('erc1155TokenId') or t.get('erc721TokenId')) for t in transfers)
        # compute approximate first/last block/timestamp if available
        first_block = transfers[0].get('blockNum') if tx_count>0 else None
        last_block = transfers[-1].get('blockNum') if tx_count>0 else None
        rows.append({
            "address": addr,
            "tx_count": tx_count,
            "unique_senders": len(senders),
            "unique_receivers": len(receivers),
            "interacts_with_contract": bool(interacts_with_contract),
            "first_block": first_block,
            "last_block": last_block
        })
    except Exception as e:
        rows.append({"address":addr, "error": str(e)})
    time.sleep(0.35)

pd.DataFrame(rows).to_csv("alchemy_summary.csv", index=False)
print("Wrote alchemy_summary.csv with", len(rows), "rows")

# Colab: 2) merge alchemy summary into annotation sheet
import pandas as pd
base = pd.read_excel("labels_for_annotation.xlsx")
try:
    alch = pd.read_csv("alchemy_summary.csv")
except:
    alch = pd.DataFrame(columns=['address'])  # empty

merged = base.merge(alch, left_on='address', right_on='address', how='left')
# add a suggested label column based on cluster mapping we discussed
cluster_map = {0:'rug_pull', 1:'phishing', 2:'ponzi', -1:'high_value_outlier'}
if 'cluster_id' in merged.columns:
    merged['cluster_suggested_label'] = merged['cluster_id'].map(cluster_map).fillna('unknown')
else:
    merged['cluster_suggested_label'] = 'unknown'

# write enriched sheet for download
merged.to_excel("labels_for_annotation_enriched.xlsx", index=False)
print("Wrote labels_for_annotation_enriched.xlsx — download this and have each of you label independently.")

import pandas as pd
alchemy = pd.read_csv("alchemy_summary.csv")
print(alchemy.shape)
print(alchemy.head(10))

# Colab cell: merge your manual annotation into final dataset and save labels_final.csv + DATASET.md
from google.colab import files
import pandas as pd, os

# --- Step 1: Upload files if not already present ---
print("If the files are already in Colab, you can skip upload prompts by cancelling them.")
print("Please upload your edited 'labels_for_annotation_enriched.xlsx' (the sheet you labeled).")
uploaded = files.upload()
if not uploaded:
    print("No files uploaded via prompt; attempting to use files already present in the working directory.")
# pick uploaded or existing file
possible_ann_names = [name for name in uploaded.keys() if name.lower().endswith(('.xlsx','.xls','.csv'))] \
                     or [f for f in os.listdir('.') if f.lower().startswith('labels_for_annotation_enriched')]
if not possible_ann_names:
    raise SystemExit("Couldn't find 'labels_for_annotation_enriched' in uploaded files or working dir. Please upload it.")
ann_name = possible_ann_names[0]
print("Using annotation file:", ann_name)

# Also find alchemy summary file
print("\nNow upload the Alchemy enrichment file (alchemy_summary.csv) if not already present.")
uploaded2 = files.upload()
alch_names = [name for name in uploaded2.keys() if name.lower().endswith('.csv')] \
             or [f for f in os.listdir('.') if 'alchemy' in f.lower() and f.lower().endswith('.csv')]
if not alch_names:
    print("No Alchemy file uploaded or found. We'll continue without it (it's optional).")
    alch = None
else:
    alch_name = alch_names[0]
    print("Using alchemy file:", alch_name)
    alch = pd.read_csv(alch_name)

# --- Step 2: Read annotation sheet ---
if ann_name.lower().endswith(('.xlsx','.xls')):
    ann = pd.read_excel(ann_name)
else:
    ann = pd.read_csv(ann_name)

print("\nAnnotation sheet columns:", ann.columns.tolist())

# normalize address column name
def find_address_col(df):
    for c in df.columns:
        if c.lower().strip() in ('address','addr'):
            return c
    return None

ann_addr_col = find_address_col(ann)
if ann_addr_col is None:
    raise SystemExit("Could not find an 'address' column in the annotation sheet. Please ensure it exists.")
ann = ann.rename(columns={ann_addr_col: 'address'})

# Ensure final_label column exists in annotation sheet
if 'final_label' not in ann.columns:
    print("annotation sheet did not have 'final_label' column — creating empty column.")
    ann['final_label'] = pd.NA

# If alchemy exists, ensure address col
if alch is not None:
    alch_addr_col = find_address_col(alch)
    if alch_addr_col is None:
        raise SystemExit("Could not find 'address' column in the alchemy enrichment file.")
    alch = alch.rename(columns={alch_addr_col: 'address'})
    print("Alchemy columns:", alch.columns.tolist())

# --- Step 3: Merge enrichment final_label (alchemy) if needed, prefer manual ann final_label ---
# If alchemy has a final_label, merge it (but manual should take precedence)
if alch is not None:
    merged = ann.merge(alch[['address', 'final_label']], on='address', how='left', suffixes=('','_alch'))
    # prefer annotation final_label if present (not null / not empty), else use alchemy's final_label
    def pick_label(row):
        a = row.get('final_label')
        b = row.get('final_label_alch')
        if pd.notna(a) and str(a).strip() != '':
            return a
        elif pd.notna(b) and str(b).strip() != '':
            return b
        else:
            return pd.NA
    merged['final_label'] = merged.apply(pick_label, axis=1)
    merged = merged.drop(columns=[c for c in merged.columns if c.endswith('_alch')])
else:
    merged = ann.copy()

# --- Step 4: Ensure required columns exist; if missing, try to pull from original Kaggle file ---
required = ['address','FLAG','candidate_labels','cluster_id','cluster_suggested_label','final_label']
for c in required:
    if c not in merged.columns:
        merged[c] = pd.NA

# Try to fill FLAG/candidate_labels from original transaction CSV if present in working dir
kaggle_path = None
for path in [
    "/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv",
    "transaction_dataset.csv",
    "transaction_dataset.csv.zip"
]:
    if os.path.exists(path):
        kaggle_path = path
        break
if kaggle_path:
    try:
        orig = pd.read_csv(kaggle_path, low_memory=False)
        # normalize address column name in orig
        addr_col_orig = find_address_col(orig)
        if addr_col_orig:
            orig = orig.rename(columns={addr_col_orig: 'address'})
            # select helpful columns
            fill_cols = []
            if 'FLAG' in orig.columns:
                merged = merged.merge(orig[['address','FLAG']], on='address', how='left', suffixes=('','_orig'))
                merged['FLAG'] = merged.apply(lambda r: r['FLAG'] if pd.notna(r['FLAG']) else r.get('FLAG_orig'), axis=1)
                if 'FLAG_orig' in merged.columns: merged = merged.drop(columns=['FLAG_orig'])
            # candidate_labels may not exist in orig; but if you have labels_candidates.csv in cwd we can merge
    except Exception as e:
        print("Could not load original transaction CSV to fill FLAG:", e)
else:
    print("Could not find original transaction CSV in standard locations; FLAG might be missing for some rows.")

# If a separate labels_candidates.csv exists, merge candidate_labels and cluster_id from it
if os.path.exists('labels_candidates_with_cluster.csv'):
    try:
        cand = pd.read_csv('labels_candidates_with_cluster.csv')
        # cand likely has 'address' and 'cluster_id' and candidate_labels
        cand_addr_col = find_address_col(cand)
        if cand_addr_col and cand_addr_col!='address':
            cand = cand.rename(columns={cand_addr_col:'address'})
        # merge where missing
        merged = merged.merge(cand[['address','candidate_labels','cluster_id']], on='address', how='left', suffixes=('','_cand'))
        merged['candidate_labels'] = merged.apply(lambda r: r['candidate_labels'] if pd.notna(r['candidate_labels']) and r['candidate_labels']!='' else r.get('candidate_labels_cand'), axis=1)
        merged['cluster_id'] = merged.apply(lambda r: r['cluster_id'] if pd.notna(r['cluster_id']) and r['cluster_id']!='' else r.get('cluster_id_cand'), axis=1)
        merged = merged.drop(columns=[c for c in merged.columns if c.endswith('_cand')])
    except Exception as e:
        print("Could not merge labels_candidates_with_cluster.csv:", e)
else:
    print("labels_candidates_with_cluster.csv not found in working dir; cluster info may be missing.")

# --- Step 5: Sanity checks and output ---
final_cols = ['address','FLAG','candidate_labels','cluster_id','cluster_suggested_label','final_label']
final = merged[final_cols].copy()

# Print distribution of final_label
print("\nFINAL LABEL DISTRIBUTION:")
print(final['final_label'].value_counts(dropna=False))

# Show number of rows missing final_label
missing_final = final['final_label'].isna().sum()
print(f"\nRows missing final_label: {missing_final} / {len(final)}")

# Per-annotator counts if annotator column exists
if 'annotator' in merged.columns:
    print("\nAnnotator counts:")
    print(merged['annotator'].value_counts())

# Save final CSV
final_csv = "labels_final.csv"
final.to_csv(final_csv, index=False)
print(f"\nSaved {final_csv} in working directory.")

# Save DATASET.md quick draft
md = f"""
# Dataset: Ethereum Fraud Types (derived)

## Source
Base dataset: Kaggle 'Ethereum Fraud Detection Dataset'.

## Files
- labels_final.csv (this file)
- labels_candidates.csv
- labels_candidates_with_cluster.csv
- labels_for_annotation_enriched.xlsx
- alchemy_summary.csv

## Notes
- 'final_label' is the human-annotated fraud type and was merged into the final dataset.
- If FLAG or candidate_labels were missing, the script attempted to fill them from labels_candidates_with_cluster.csv or the original transaction CSV where available.
"""
with open("DATASET.md","w") as f:
    f.write(md.strip())

print("Wrote DATASET.md. Done.")

import pandas as pd

# Load annotated sheet
ann = pd.read_excel("labels_for_annotation_enriched.xlsx")

# Ensure numeric columns exist
num_cols = [
    "Sent tnx", "Received Tnx", "Unique Received From Addresses",
    "Number of Created Contracts", "Time Diff between first and last (Mins)",
    "ERC20 uniq sent token name", "Avg min between sent tnx", "Avg min between received tnx"
]
for c in num_cols:
    if c not in ann.columns:
        print(f"Warning: {c} not in file!")

# Function to assign labels based on features + clusters
def auto_label(row):
    cand = str(row.get("candidate_labels", "")).lower()
    cluster = row.get("cluster_id", None)
    flag = row.get("FLAG", 1) if "FLAG" in row else 1

    # Benign case
    if flag == 0:
        return "benign"

    # --- Heuristic rules (strongest first) ---
    if row.get("Unique Received From Addresses", 0) >= 30 and row.get("Sent tnx", 0) <= 5:
        return "phishing"
    if row.get("Received Tnx", 0) >= 200 and row.get("Sent tnx", 0) >= 50 and row.get("Time Diff between first and last (Mins)", 0) > 24*60:
        return "ponzi"
    if row.get("Number of Created Contracts", 0) >= 1 and row.get("ERC20 max val sent contract", 0) > 1e5:
        return "rug_pull"
    if row.get("ERC20 uniq sent token name", 99) <= 3 and (row.get("Sent tnx", 0) + row.get("Received Tnx", 0)) >= 100:
        return "wash"
    if (row.get("Sent tnx", 0) + row.get("Received Tnx", 0)) > 100 and row.get("Avg min between sent tnx", 1e9) < 60:
        return "mixer"

    # --- Cluster-informed defaults (if heuristics didn’t fire) ---
    if cluster == 1:
        return "phishing"
    if cluster == 2:
        return "ponzi"
    if cluster == 0:
        return "rug_pull"
    if cluster == -1:
        return "mixer"

    # Fallback
    return "unknown"

# Apply labeling
ann["final_label"] = ann.apply(auto_label, axis=1)

# Save
ann.to_csv("labels_final.csv", index=False)

# Distribution check
print("\nLabel distribution:")
print(ann["final_label"].value_counts())

from google.colab import files
files.download("/kaggle/input/ethereum-frauddetection-dataset/transaction_dataset.csv")

from google.colab import files
uploaded = files.upload()  # choose transaction_dataset.csv from your computer

import pandas as pd

# Load files
ann = pd.read_excel("labels_for_annotation_enriched.xlsx")
orig = pd.read_csv("transaction_dataset.csv", low_memory=False)

# Normalize address columns
if "address" not in ann.columns:
    ann = ann.rename(columns={ann.columns[0]: "address"})
if "Address" in orig.columns:
    orig = orig.rename(columns={"Address": "address"})

# Force address columns to string (so merge works)
ann["address"] = ann["address"].astype(str)
orig["address"] = orig["address"].astype(str)

# Merge
merged = ann.merge(orig, on="address", how="left", suffixes=('', '_orig'))

# --- Heuristic rules ---
def is_phishing(r):
    return (r.get("Unique Received From Addresses",0) >= 30) and (r.get("Sent tnx",0) <= 5)

def is_ponzi(r):
    return (r.get("Received Tnx",0) >= 200) and (r.get("Sent tnx",0) >= 50) and (r.get("Time Diff between first and last (Mins)",0) > 24*60)

def is_rug(r):
    return (r.get("Number of Created Contracts",0) >= 1) and (r.get("ERC20 max val sent contract",0) > 1e5)

def is_wash(r):
    total = (r.get("Sent tnx",0) + r.get("Received Tnx",0))
    uniq = r.get("ERC20 uniq sent token name", 99)
    return (uniq <= 3) and (total >= 100)

def is_mixer(r):
    txsum = r.get("Sent tnx",0) + r.get("Received Tnx",0)
    return (txsum > 100) and (r.get("Avg min between sent tnx",1e9) < 60)

# Assign labels
def assign_label(row):
    if row.get("FLAG",1) == 0:
        return "benign"
    if is_phishing(row): return "phishing"
    if is_ponzi(row): return "ponzi"
    if is_rug(row): return "rug_pull"
    if is_wash(row): return "wash"
    if is_mixer(row): return "mixer"
    if row.get("cluster_id") == 0: return "rug_pull"
    if row.get("cluster_id") == 1: return "phishing"
    if row.get("cluster_id") == 2: return "ponzi"
    if row.get("cluster_id") == -1: return "mixer"
    return "unknown"

merged["final_label"] = merged.apply(assign_label, axis=1)

# Save final dataset
out_cols = ["address","FLAG","candidate_labels","cluster_id","cluster_suggested_label","final_label"]
final = merged[[c for c in out_cols if c in merged.columns]].copy()
final.to_csv("labels_final.csv", index=False)

# Report
print("✅ labels_final.csv saved")
print("\nLabel distribution:")
print(final["final_label"].value_counts())

print("\nSample rows:")
print(final.head(10).to_string(index=False))

# Save dataset summary
with open("DATASET.md","w") as f:
    f.write("# DATASET.md\nFraud types assigned using heuristics + clustering consistency.\n")

# Cell A: pick ambiguous addresses and create to_review_for_alchemy.xlsx
import pandas as pd, os

# Paths (adjust if files named differently)
ann_file = "labels_for_annotation_enriched.xlsx"   # main enriched annotation sheet you used earlier
alchemy_file = "alchemy_summary.csv"               # enrichment summary you already generated
labels_final_file = "labels_final.csv"             # if already produced

# Load files (try multiple fallbacks)
if os.path.exists(labels_final_file):
    df_final = pd.read_csv(labels_final_file, dtype=str)
else:
    # fallback to annotation sheet
    df_final = pd.read_excel(ann_file, dtype=str)

alch = pd.read_csv(alchemy_file, dtype=str) if os.path.exists(alchemy_file) else pd.DataFrame()

# normalize columns to expected names (lower-case helpers)
df_final = df_final.rename(columns={c: c.strip() for c in df_final.columns})
if 'address' not in df_final.columns:
    # try first column if address wasn't named
    df_final = df_final.rename(columns={df_final.columns[0]:'address'})

# Ensure key columns exist
for col in ['candidate_labels','cluster_id','cluster_suggested_label','final_label']:
    if col not in df_final.columns:
        df_final[col] = pd.NA

# Mark ambiguous: unknown candidate OR final_label NA/unknown OR cluster -1
def is_amb(row):
    cand = str(row.get('candidate_labels','')).lower()
    fl = str(row.get('final_label','')).lower()
    cl = row.get('cluster_id')
    cond = (('unknown' in cand) or (fl in ('nan','na','none','', 'unknown')) or (str(cl).strip() in ['-1','-1.0']))
    return cond

df_final['ambiguous_flag'] = df_final.apply(is_amb, axis=1)

# Prefer ambiguous fraud rows only (FLAG==1) if FLAG exists
if 'FLAG' in df_final.columns:
    df_work = df_final[(df_final['ambiguous_flag']==True) & (df_final['FLAG'].astype(str)=='1')].copy()
else:
    df_work = df_final[df_final['ambiguous_flag']==True].copy()

# Join alchemy summary (if present)
if not alch.empty:
    # normalize alchemy address column
    alch = alch.rename(columns={c:c.strip() for c in alch.columns})
    if 'address' not in alch.columns:
        alch = alch.rename(columns={alch.columns[0]:'address'})
    merged = df_work.merge(alch, on='address', how='left', suffixes=('','_alch'))
else:
    merged = df_work.copy()

# Order by usefulness: prefer rows where alchemy exists, then high tx_count if available
def safe_int(x):
    try:
        return int(float(x))
    except:
        return 0

if 'tx_count' in merged.columns:
    merged['tx_count_num'] = merged['tx_count'].apply(safe_int)
else:
    merged['tx_count_num'] = 0

merged = merged.sort_values(by=['tx_count_num'], ascending=False)

# Select top N ambiguous (30 default)
N = 30
to_review = merged.head(N).copy()

# Build review sheet columns (keep a compact helpful set)
keep = ['address','candidate_labels','cluster_id','cluster_suggested_label','final_label',
        'tx_count','unique_senders','unique_receivers','interacts_with_contract','first_block','last_block']
# add any of those that exist in to_review
keep_existing = [c for c in keep if c in to_review.columns]
# Add editable note columns
to_review_sheet = to_review[keep_existing].copy()
to_review_sheet['alchemy_notes'] = pd.NA
to_review_sheet['final_label_updated'] = pd.NA
to_review_sheet['review_reason'] = pd.NA

out_name = "to_review_for_alchemy.xlsx"
to_review_sheet.to_excel(out_name, index=False)
print(f"WROTE {out_name} with {len(to_review_sheet)} rows. Download it, edit `alchemy_notes` and/or `final_label_updated` for rows you want to change, then re-upload using Cell B.")
print("\nFirst rows preview:")
print(to_review_sheet.head(10).to_string(index=False))

# Safe patch + optional full auto-labeling
import pandas as pd, os

# file paths (adjust if different)
big_fp = "labels_final_with_notes.csv"   # your big skeleton
sample_fp = "to_review_for_alchemy_auto.csv"  # auto-labeled 30-row file

# load
big = pd.read_csv(big_fp, dtype=str)
sample = pd.read_csv(sample_fp, dtype=str)

# normalize address column names
def norm_address_col(df):
    for c in df.columns:
        if c.lower().strip() in ("address","addr"):
            df.rename(columns={c:"address"}, inplace=True)
            return
    # if not found, assume first col is address
    df.rename(columns={df.columns[0]:"address"}, inplace=True)

norm_address_col(big)
norm_address_col(sample)

# ensure sample has final_label_updated (if not, check final_label)
if 'final_label_updated' not in sample.columns and 'final_label' in sample.columns:
    sample['final_label_updated'] = sample['final_label']

# Prepare only the update columns that actually exist
update_cols = ['address']
for c in ['final_label_updated','alchemy_notes','auto_confidence','review_reason']:
    if c in sample.columns:
        update_cols.append(c)

updates = sample[update_cols].copy()

# merge safely (left join big <- updates)
patched = big.merge(updates, on='address', how='left', suffixes=('','_upd'))

# If final_label column missing in big, create it
if 'final_label' not in patched.columns:
    patched['final_label'] = pd.NA

# Replace final_label where final_label_updated exists
def apply_update(row):
    upd = row.get('final_label_updated')
    if pd.notna(upd) and str(upd).strip() != '':
        return upd
    return row.get('final_label')

patched['final_label_before'] = patched['final_label']
patched['final_label'] = patched.apply(apply_update, axis=1)

# Merge notes/confidence safely: prefer existing, else use *_upd
for col in ['alchemy_notes','auto_confidence','review_reason']:
    upd_col = col + '_upd'
    if upd_col in patched.columns:
        if col not in patched.columns:
            patched[col] = pd.NA
        patched[col] = patched[col].fillna(patched[upd_col])
        # drop the _upd later

# drop helper update cols
drop_candidates = [c for c in patched.columns if c.endswith('_upd') or c=='final_label_updated']
patched.drop(columns=[c for c in drop_candidates if c in patched.columns], inplace=True)

# save patched file
patched_fp = "labels_final_patched.csv"
patched.to_csv(patched_fp, index=False)
print(f"✅ WROTE {patched_fp}")

# report how many rows got labels
total = len(patched)
labeled = patched['final_label'].notna().sum()
print(f"Labeled rows: {labeled} / {total}")

print("\nTop label counts:")
print(patched['final_label'].value_counts(dropna=False).head(20))

# show first few updated rows where final_label changed
changed = patched[patched['final_label_before'].fillna('') != patched['final_label'].fillna('')]
print(f"\nRows changed by patch: {len(changed)} (showing up to 10):")
print(changed[['address','final_label_before','final_label']].head(10).to_string(index=False))

# === OPTIONAL: run auto-label rules across all remaining unlabeled rows ===
auto_label_all = True   # <- set to True to auto-label the rest; set False if you prefer manual + small patch
if auto_label_all:
    print("\nAUTO-LABELING remaining rows using defensible rules (this will overwrite only blanks).")
    df = patched

    # helper to coerce numeric safely from merged dataset (some numeric columns might have other names)
    def safe_num(df, possibles, default=0):
        for p in possibles:
            for c in df.columns:
                if c.lower().strip()==p.lower().strip():
                    return pd.to_numeric(df[c], errors='coerce').fillna(default)
        return pd.Series([default]*len(df))

    # candidate numeric lookups (names you have / that may appear)
    Sent = safe_num(df, ["Sent tnx","sent tnx","sent_txn","sent_txns","sent_tx"], default=0)
    Recv = safe_num(df, ["Received Tnx","received tnx","received_txn","received_txns"], default=0)
    UniqueRecv = safe_num(df, ["Unique Received From Addresses","unique received from addresses","unique_received_from_addresses"], default=0)
    NumCreated = safe_num(df, ["Number of Created Contracts","number of created contracts","created contracts"], default=0)
    ERC20uniq = safe_num(df, ["ERC20 uniq sent token name","erc20 uniq sent token name"], default=99)
    AvgMinSent = safe_num(df, ["Avg min between sent tnx","avg min between sent tnx"], default=1e9)
    # If tx_count exists from Alchemy merged
    tx_count = safe_num(df, ["tx_count","tx_count_num"], default=0)

    # apply rules only where final_label is empty
    mask_blank = df['final_label'].isna() | (df['final_label'].astype(str).str.strip()=='')
    # build labels
    labels = df['final_label'].astype(object).copy()
    # phishing
    p_mask = (mask_blank) & (UniqueRecv >= 30) & (Sent <= 5)
    labels[p_mask] = "phishing"
    # ponzi
    ponzi_mask = (mask_blank) & (Recv >= 200) & (Sent >= 50)
    labels[ponzi_mask] = "ponzi"
    # rug_pull
    rug_mask = (mask_blank) & (NumCreated >= 1) & (df.get('ERC20 max val sent contract', pd.Series([0]*len(df))).fillna(0).astype(float) > 1e5)
    labels[rug_mask] = "rug_pull"
    # wash
    wash_mask = (mask_blank) & (ERC20uniq <= 3) & ((Sent + Recv) >= 100)
    labels[wash_mask] = "wash"
    # mixer
    mixer_mask = (mask_blank) & ((Sent + Recv) > 100) & (AvgMinSent < 60)
    labels[mixer_mask] = "mixer"
    # fallback cluster mapping for anything still blank
    still_blank = (labels.isna()) | (labels.astype(str).str.strip()=='')
    labels[still_blank & (df['cluster_id'].astype(str).isin(['0','0.0']))] = "rug_pull"
    labels[still_blank & (df['cluster_id'].astype(str).isin(['1','1.0']))] = "phishing"
    labels[still_blank & (df['cluster_id'].astype(str).isin(['2','2.0']))] = "ponzi"
    labels[still_blank & (df['cluster_id'].astype(str).isin(['-1','-1.0']))] = "mixer"
    labels[labels.isna()] = "unknown"

    df['final_label'] = labels
    df.to_csv("labels_final_patched_autolabeled.csv", index=False)
    print("✅ WROTE labels_final_patched_autolabeled.csv")
    print("New distribution (top counts):")
    print(df['final_label'].value_counts().head(20))

print("\nAll done. If anything looks off, paste the top label counts here and I'll interpret.")

import pandas as pd

# Load your patched + auto-labeled dataset
df = pd.read_csv("labels_final_patched_autolabeled.csv")

print("Total rows:", len(df))
print("\nLabel distribution:")
print(df['final_label'].value_counts(dropna=False))

print("\nAny missing labels? ->", df['final_label'].isna().sum())

# Show 2 sample addresses per label to verify
print("\nSample rows per class:")
print(df.groupby('final_label').head(2)[['address','final_label']])

"""DAY 2"""

# Cell 0: Setup & quick checks (Colab default path)

import sys, os, pandas as pd

print("Python:", sys.version)
print("CWD:", os.getcwd())
print("Files in current dir:", os.listdir('.')[:50])  # current working dir

# Adjust paths (everything is in /content by default)
labels_path = "labels_final_patched_autolabeled.csv"   # uploaded Day 1 result
kaggle_path = "transaction_dataset.csv"               # uploaded Kaggle dataset

# Sanity checks
assert os.path.exists(labels_path), f"labels file not found at {labels_path}"
assert os.path.exists(kaggle_path), f"kaggle transaction CSV not found at {kaggle_path}"

labels = pd.read_csv(labels_path, dtype=str)
orig = pd.read_csv(kaggle_path, low_memory=False, dtype=str)

print("\nlabels_final_patched_autolabeled.csv:", labels.shape)
print(labels['final_label'].value_counts(dropna=False))

print("\ntransaction_dataset.csv:", orig.shape)
print("Sample columns:", orig.columns.tolist()[:20])

# Merge for check
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
if 'address' not in orig.columns:
    orig = orig.rename(columns={orig.columns[0]:'address'})

merged = labels.merge(orig, on='address', how='left', suffixes=('','_orig'))
print("\nMerged shape:", merged.shape)
print("Merged sample rows:")
display(merged.head(5))

# Drop rows with labels that have < 2 samples (like "other")
label_counts = df['final_label_collapsed'].value_counts()
valid_labels = label_counts[label_counts >= 2].index
df = df[df['final_label_collapsed'].isin(valid_labels)].reset_index(drop=True)

print("Label frequencies after dropping rare:", df['final_label_collapsed'].value_counts().to_dict())

# Rebuild features + labels
X = df[num_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)
y = le.fit_transform(df['final_label_collapsed'].astype(str))

# Stratified split (now safe)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

print("Split sizes:", X_train.shape, X_test.shape)
print("Classes in y_train:", np.unique(y_train, return_counts=True))
print("Classes in y_test:", np.unique(y_test, return_counts=True))

# Make sure artifacts folder exists
os.makedirs("artifacts", exist_ok=True)

# Cell 2: Train baseline models (Logistic, RF, XGB)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, average_precision_score, precision_recall_fscore_support
import joblib, json

try:
    import xgboost as xgb
    has_xgb = True
except ImportError:
    has_xgb = False

results = {}

def evaluate_model(model, name, X_train, y_train, X_test, y_test, scale=False):
    if scale:
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler().fit(X_train)
        X_train = scaler.transform(X_train)
        X_test = scaler.transform(X_test)
        joblib.dump(scaler, f"artifacts/{name}_scaler.joblib")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="macro")
    report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)

    # per-class average precision (if proba exists)
    ap = {}
    if y_prob is not None:
        for i, cls in enumerate(le.classes_):
            ap[cls] = average_precision_score((y_test==i).astype(int), y_prob[:, i])

    results[name] = {"acc": acc, "f1_macro": f1, "ap": ap, "report": report}
    joblib.dump(model, f"artifacts/{name}.joblib")
    print(f"\n{name.upper()} RESULTS")
    print("Accuracy:", acc, "F1_macro:", f1)
    print(classification_report(y_test, y_pred, target_names=le.classes_))

# Logistic Regression (scaled)
lr = LogisticRegression(max_iter=500, solver="saga", multi_class="multinomial")
evaluate_model(lr, "logistic", X_train, y_train, X_test, y_test, scale=True)

# Random Forest
rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)
evaluate_model(rf, "rf", X_train, y_train, X_test, y_test, scale=False)

# XGBoost (if installed)
if has_xgb:
    xgb_model = xgb.XGBClassifier(eval_metric="mlogloss", use_label_encoder=False, n_jobs=-1)
    evaluate_model(xgb_model, "xgb", X_train, y_train, X_test, y_test, scale=False)
else:
    print("⚠️ XGBoost not available in this Colab runtime.")

# Save metrics
os.makedirs("artifacts", exist_ok=True)
with open("artifacts/metrics_baselines.json", "w") as f:
    json.dump(results, f, indent=2)

print("\n✅ Saved metrics to artifacts/metrics_baselines.json")

# Cell 2: Train baseline models (Logistic, RF, XGB)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, average_precision_score
import joblib, json, os

# ✅ ensure artifacts folder
os.makedirs("artifacts", exist_ok=True)

try:
    import xgboost as xgb
    has_xgb = True
except ImportError:
    has_xgb = False

# Cell 2: Train baseline models (Logistic, RF, XGB)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, average_precision_score, precision_recall_fscore_support
import joblib, json

try:
    import xgboost as xgb
    has_xgb = True
except ImportError:
    has_xgb = False

results = {}

def evaluate_model(model, name, X_train, y_train, X_test, y_test, scale=False):
    if scale:
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler().fit(X_train)
        X_train = scaler.transform(X_train)
        X_test = scaler.transform(X_test)
        joblib.dump(scaler, f"artifacts/{name}_scaler.joblib")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="macro")
    report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)

    # per-class average precision (if proba exists)
    ap = {}
    if y_prob is not None:
        for i, cls in enumerate(le.classes_):
            ap[cls] = average_precision_score((y_test==i).astype(int), y_prob[:, i])

    results[name] = {"acc": acc, "f1_macro": f1, "ap": ap, "report": report}
    joblib.dump(model, f"artifacts/{name}.joblib")
    print(f"\n{name.upper()} RESULTS")
    print("Accuracy:", acc, "F1_macro:", f1)
    print(classification_report(y_test, y_pred, target_names=le.classes_))

# Logistic Regression (scaled)
lr = LogisticRegression(max_iter=500, solver="saga", multi_class="multinomial")
evaluate_model(lr, "logistic", X_train, y_train, X_test, y_test, scale=True)

# Random Forest
rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)
evaluate_model(rf, "rf", X_train, y_train, X_test, y_test, scale=False)

# XGBoost (if installed)
if has_xgb:
    xgb_model = xgb.XGBClassifier(eval_metric="mlogloss", use_label_encoder=False, n_jobs=-1)
    evaluate_model(xgb_model, "xgb", X_train, y_train, X_test, y_test, scale=False)
else:
    print("⚠️ XGBoost not available in this Colab runtime.")

# Save metrics
os.makedirs("artifacts", exist_ok=True)
with open("artifacts/metrics_baselines.json", "w") as f:
    json.dump(results, f, indent=2)

print("\n✅ Saved metrics to artifacts/metrics_baselines.json")

!pip install gensim -q

# Node2Vec structural prior (fast; no PyG needed)
# Run this in your Colab cell (it uses networkx + gensim; both are usually preinstalled)

import pandas as pd, numpy as np, json, random
import networkx as nx
from gensim.models import Word2Vec

# Load merged dataset
labels = pd.read_csv("labels_final_patched_autolabeled.csv", dtype=str)
orig = pd.read_csv("transaction_dataset.csv", low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# Select numeric feature columns for similarity (safe fallback)
candidate_cols = ['Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses',
                  'Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
                  'ERC20 uniq sent token name','ERC20 uniq rec token name','ERC20 avg val sent','ERC20 avg val rec']
feat_cols = [c for c in df.columns if c in candidate_cols]
if len(feat_cols) < 4:
    tmp = df.copy()
    for c in tmp.columns:
        tmp[c] = pd.to_numeric(tmp[c].astype(str).str.replace(',',''), errors='coerce')
    feat_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()[:12]

print("Feature cols used for kNN graph:", feat_cols)

# Build kNN graph (k=8)
from sklearn.neighbors import NearestNeighbors
X = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values
k = 8
nn = NearestNeighbors(n_neighbors=min(k+1, len(X)-1), metric='cosine', n_jobs=-1).fit(X)
distances, indices = nn.kneighbors(X)
N = len(df)
edges = set()
for i in range(N):
    for j_idx in indices[i,1:]:
        a,b = int(i), int(j_idx)
        if a==b: continue
        if a<b: edges.add((a,b))
        else: edges.add((b,a))
edges = sorted(list(edges))
print("Nodes:", N, "Edges:", len(edges))

# Build NetworkX graph with address labels
node_map = {i: df.loc[i,'address'] for i in range(N)}
G = nx.Graph()
for u,v in edges:
    G.add_edge(node_map[u], node_map[v])

# Random walks
def random_walk(G, start, walk_length):
    walk = [start]
    for _ in range(walk_length-1):
        cur = walk[-1]
        neigh = list(G.neighbors(cur))
        if not neigh:
            break
        walk.append(random.choice(neigh))
    return walk

walks = []
walks_per_node = 10
walk_length = 20
nodes = list(G.nodes())
for n in nodes:
    for _ in range(walks_per_node):
        walks.append(random_walk(G, n, walk_length))

print("Generated", len(walks), "walks")

# Train Word2Vec (Node2Vec-ish)
model = Word2Vec(walks, vector_size=128, window=5, min_count=0, sg=1, epochs=2)

# Build embedding matrix aligned to df rows
emb_dim = model.vector_size
emb = np.zeros((N, emb_dim), dtype=float)
for i in range(N):
    addr = node_map[i]
    if addr in model.wv:
        emb[i] = model.wv[addr]
    else:
        emb[i] = np.random.normal(scale=0.01, size=(emb_dim,))

# Save
np.save("artifacts/structural_embeddings_node2vec.npy", emb)
with open("artifacts/node_map.json","w") as f:
    json.dump(node_map, f)

print("Saved embeddings -> artifacts/structural_embeddings_node2vec.npy (shape {})".format(emb.shape))

# Fast structural embeddings via adjacency SVD (no gensim / no PyG)
import pandas as pd, numpy as np, json, os
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import TruncatedSVD
from scipy import sparse

# paths
labels_fp = "labels_final_patched_autolabeled.csv"
orig_fp = "transaction_dataset.csv"
os.makedirs("artifacts", exist_ok=True)

# load & merge
labels = pd.read_csv(labels_fp, dtype=str)
orig = pd.read_csv(orig_fp, low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# choose numeric features (fallback safe)
candidate_cols = ['Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses',
                  'Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
                  'ERC20 uniq sent token name','ERC20 uniq rec token name','ERC20 avg val sent','ERC20 avg val rec']
feat_cols = [c for c in df.columns if c in candidate_cols]
if len(feat_cols) < 4:
    tmp = df.copy()
    for c in tmp.columns:
        tmp[c] = pd.to_numeric(tmp[c].astype(str).str.replace(',',''), errors='coerce')
    feat_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()[:12]

print("Feature cols for kNN graph:", feat_cols)

# build feature matrix
X = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values
N = X.shape[0]

# build kNN graph
k = 8
nn = NearestNeighbors(n_neighbors=min(k+1, N-1), metric='cosine', n_jobs=-1).fit(X)
dists, inds = nn.kneighbors(X)
rows = []
cols = []
data = []
for i in range(N):
    for j in inds[i,1:]:
        rows.append(i)
        cols.append(int(j))
        data.append(1.0)
# make symmetric
rows_sym = rows + cols
cols_sym = cols + rows
data_sym = data + data
A = sparse.coo_matrix((data_sym, (rows_sym, cols_sym)), shape=(N,N)).tocsr()
print("Built adjacency matrix: nodes", N, "edges (approx):", A.nnz//2)

# Optionally normalize (random-walk)
deg = np.array(A.sum(axis=1)).flatten()
deg_inv = np.power(deg, -0.5, where=deg>0)
D_inv = sparse.diags(deg_inv)
A_norm = D_inv.dot(A).dot(D_inv)  # symmetric normalized adjacency

# Truncated SVD on normalized adjacency (spectral-like)
embed_dim = 128
svd = TruncatedSVD(n_components=embed_dim, random_state=42)
Z = svd.fit_transform(A_norm)  # shape (N, embed_dim)

# Save
np.save("artifacts/structural_embeddings_svd.npy", Z)
node_map = {i: df.loc[i,'address'] for i in range(N)}
with open("artifacts/node_map.json","w") as f:
    json.dump(node_map, f)

print("Saved embeddings -> artifacts/structural_embeddings_svd.npy (shape {})".format(Z.shape))
print("Sample embedding (node 0, first 8 dims):", Z[0,:8].tolist())

# Hybrid model: structural embeddings (SVD) + tabular features -> small MLP
import numpy as np, pandas as pd, json, os, joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report

# Paths
labels_fp = "labels_final_patched_autolabeled.csv"
orig_fp = "transaction_dataset.csv"
emb_fp = "artifacts/structural_embeddings_svd.npy"
node_map_fp = "artifacts/node_map.json"
os.makedirs("artifacts", exist_ok=True)

# Load
labels = pd.read_csv(labels_fp, dtype=str)
orig = pd.read_csv(orig_fp, low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# Load embeddings (aligned to df row order because node_map used df indexes)
emb = np.load(emb_fp)  # shape (N, D)
print("Loaded embeddings shape:", emb.shape)

# Tabular feature selection (reuse features used earlier; fallback to numeric)
candidate_cols = ['Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
                  'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses']
feat_cols = [c for c in df.columns if c in candidate_cols]
if len(feat_cols) < 4:
    tmp = df.copy()
    for c in tmp.columns:
        tmp[c] = pd.to_numeric(tmp[c].astype(str).str.replace(',',''), errors='coerce')
    feat_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()[:8]

print("Tabular features used:", feat_cols)

X_tab = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values
X = np.hstack([X_tab, emb])  # concat tabular + structural
y_raw = df['final_label'].astype(str).values

# Optional: drop rows if any NaN or mismatch shape
N = len(df)
if X.shape[0] != N:
    raise RuntimeError(f"Row mismatch: X {X.shape[0]} vs df {N}")

# Encode labels (drop any tiny-class that we removed earlier)
le = LabelEncoder()
y = le.fit_transform(y_raw)

# Train/test split (stratified)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Scale
scaler = StandardScaler().fit(X_train)
X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
joblib.dump(scaler, "artifacts/hybrid_scaler.joblib")

# Train small MLP
clf = MLPClassifier(hidden_layer_sizes=(256,128), max_iter=300, random_state=42)
clf.fit(X_train_s, y_train)

# Eval
y_pred = clf.predict(X_test_s)
acc = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')
prec, rec, f1s, sup = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)
report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)

metrics = {"acc": acc, "f1_macro": f1_macro, "per_class_precision": dict(zip(le.classes_, prec.tolist())),
           "per_class_recall": dict(zip(le.classes_, rec.tolist())), "report": report}

# Save
joblib.dump(clf, "artifacts/hybrid_mlp.joblib")
with open("artifacts/metrics_hybrid.json","w") as f:
    json.dump(metrics, f, indent=2)

print("Hybrid metrics (saved to artifacts/metrics_hybrid.json):")
print("Accuracy:", acc)
print("F1_macro:", f1_macro)
print("\nClassification report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# --- Handle rare labels ---
from collections import Counter

label_counts = Counter(y_raw)
print("Label counts before filtering:", label_counts)

# Keep only classes with >= 5 samples (you can raise/lower threshold)
mask = [label_counts[label] >= 5 for label in y_raw]
X = X[mask]
y_raw = y_raw[mask]

print("After filtering, rows:", X.shape[0])

# Re-encode labels
y = le.fit_transform(y_raw)

# Stratified split now safe
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

print("Train/Test sizes:", X_train.shape, X_test.shape)
print("Classes:", le.classes_)

# Train & evaluate hybrid MLP (final step)
import joblib, json
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support

# (If you restarted kernel, reload X,y and embeddings first; otherwise proceed)
# X, y, X_train, X_test, y_train, y_test are assumed to exist from previous cell.

# Scale features
scaler = StandardScaler().fit(X_train)
X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
joblib.dump(scaler, "artifacts/hybrid_final_scaler.joblib")

# Train MLP
clf = MLPClassifier(hidden_layer_sizes=(256,128), max_iter=300, random_state=42)
clf.fit(X_train_s, y_train)

# Eval
y_pred = clf.predict(X_test_s)
acc = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')
report = classification_report(y_test, y_pred, target_names=list(le.classes_), output_dict=True)
prec, rec, f1s, sup = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)

metrics = {
    "acc": acc,
    "f1_macro": f1_macro,
    "per_class_precision": dict(zip(le.classes_, prec.tolist())),
    "per_class_recall": dict(zip(le.classes_, rec.tolist())),
    "classification_report": report
}

# Save model + metrics
joblib.dump(clf, "artifacts/hybrid_final_mlp.joblib")
with open("artifacts/metrics_hybrid_final.json","w") as f:
    json.dump(metrics, f, indent=2)

print("✅ Hybrid training finished and saved.")
print("Accuracy:", acc)
print("F1_macro:", f1_macro)
print("\nClassification report:\n")
from pprint import pprint
pprint(report)

# Cell A: Confusion matrix + PR curves for baseline (xgb if available) and hybrid
import json, os, joblib, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

os.makedirs("artifacts/plots", exist_ok=True)

# Load data used for evaluation (recreate test split to get y_test and predictions)
labels = pd.read_csv("labels_final_patched_autolabeled.csv", dtype=str)
orig = pd.read_csv("transaction_dataset.csv", low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# tabular features used earlier
candidate_cols = ['Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
                  'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses']
feat_cols = [c for c in df.columns if c in candidate_cols]
if len(feat_cols) < 4:
    tmp = df.copy()
    for c in tmp.columns:
        tmp[c] = pd.to_numeric(tmp[c].astype(str).str.replace(',',''), errors='coerce')
    feat_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()[:8]

# load structural embeddings and concat (if exists)
emb_path = "artifacts/structural_embeddings_svd.npy"
if os.path.exists(emb_path):
    emb = np.load(emb_path)
else:
    emb = None

X_tab = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values
if emb is not None and emb.shape[0]==len(df):
    X_hybrid = np.hstack([X_tab, emb])
else:
    X_hybrid = X_tab.copy()

# Filter to classes used in training (drop tiny class if present)
from collections import Counter
label_counts = Counter(df['final_label'].astype(str))
valid_labels = [lbl for lbl,cnt in label_counts.items() if cnt >= 5]
mask = df['final_label'].astype(str).isin(valid_labels)
X_tab = X_tab[mask.values]
X_hybrid = X_hybrid[mask.values]
y_raw = df['final_label'].astype(str)[mask.values].values

le = LabelEncoder()
y = le.fit_transform(y_raw)

# train/test split (same random_state used earlier)
X_train_tab, X_test_tab, y_train, y_test = train_test_split(X_tab, y, stratify=y, test_size=0.2, random_state=42)
X_train_h, X_test_h, _, _ = train_test_split(X_hybrid, y, stratify=y, test_size=0.2, random_state=42)

# Load models if available
models = {}
for name in ["xgb","rf","logistic","hybrid_final_mlp","hybrid_mlp","xgboost"]:
    p = f"artifacts/{name}.joblib"
    if os.path.exists(p):
        try:
            models[name] = joblib.load(p)
        except:
            pass

# prefer xgboost or xgb; hybrid model is artifacts/hybrid_final_mlp.joblib or artifacts/hybrid_mlp.joblib
pref_tab_model = None
for cand in ["xgb","xgboost","rf","logistic"]:
    if cand in models:
        pref_tab_model = models[cand]
        pref_tab_name = cand
        break

pref_hybrid_model = None
for cand in ["hybrid_final_mlp","hybrid_mlp"]:
    if cand in models:
        pref_hybrid_model = models[cand]
        pref_hybrid_name = cand
        break

print("Using tabular model:", pref_tab_name if pref_tab_model is not None else "NONE")
print("Using hybrid model:", pref_hybrid_name if pref_hybrid_model is not None else "NONE")

# Evaluate and plot confusion matrix for each available model
def plot_confusion(mdl, X_test, y_test, title, fname):
    y_pred = mdl.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)
    ax.set_title(title)
    plt.tight_layout()
    fig.savefig(fname, dpi=200)
    plt.close(fig)

if pref_tab_model is not None:
    # ensure scaling if scalers exist
    scaler_tab = "artifacts/scaler.joblib"
    if os.path.exists(scaler_tab):
        sc = joblib.load(scaler_tab)
        X_test_tab_used = sc.transform(X_test_tab)
    else:
        X_test_tab_used = X_test_tab
    plot_confusion(pref_tab_model, X_test_tab_used, y_test, f"Confusion: {pref_tab_name}", "artifacts/plots/confusion_tabular.png")
    print("Saved artifacts/plots/confusion_tabular.png")

if pref_hybrid_model is not None:
    scaler_h = "artifacts/hybrid_final_scaler.joblib"
    if os.path.exists(scaler_h):
        sc_h = joblib.load(scaler_h)
        X_test_h_used = sc_h.transform(X_test_h)
    else:
        X_test_h_used = X_test_h
    plot_confusion(pref_hybrid_model, X_test_h_used, y_test, f"Confusion: {pref_hybrid_name}", "artifacts/plots/confusion_hybrid.png")
    print("Saved artifacts/plots/confusion_hybrid.png")

# PR curves for hybrid if probabilities available
def plot_pr_curve(mdl, X_test, y_test, title, fname):
    if not hasattr(mdl, "predict_proba"):
        return
    y_prob = mdl.predict_proba(X_test)
    plt.figure(figsize=(6,5))
    for i, cls in enumerate(le.classes_):
        y_true = (y_test==i).astype(int)
        precision, recall, _ = precision_recall_curve(y_true, y_prob[:,i])
        ap = average_precision_score(y_true, y_prob[:,i])
        plt.plot(recall, precision, label=f"{cls} (AP={ap:.2f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(title)
    plt.legend(loc="lower left", fontsize="small")
    plt.tight_layout()
    plt.savefig(fname, dpi=200)
    plt.close()
    print("Saved", fname)

if pref_tab_model is not None:
    plot_pr_curve(pref_tab_model, X_test_tab_used, y_test, f"PR Curve: {pref_tab_name}", "artifacts/plots/pr_tabular.png")
if pref_hybrid_model is not None:
    plot_pr_curve(pref_hybrid_model, X_test_h_used, y_test, f"PR Curve: {pref_hybrid_name}", "artifacts/plots/pr_hybrid.png")

print("Plots done. Check artifacts/plots/")

# Fix: reconstruct feature matrix to match baseline model expected feature count,
# then plot confusion matrix. Run this in Colab.

import os, joblib, numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load df and models (reuse variables if in session)
labels = pd.read_csv("labels_final_patched_autolabeled.csv", dtype=str)
orig = pd.read_csv("transaction_dataset.csv", low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# Filter to valid labels (>=5) used earlier
from collections import Counter
label_counts = Counter(df['final_label'].astype(str))
valid_labels = [lbl for lbl,cnt in label_counts.items() if cnt >= 5]
mask = df['final_label'].astype(str).isin(valid_labels)
df_masked = df[mask].reset_index(drop=True)
print("Rows after masking rare labels:", len(df_masked))

# Load preferred tabular model
import joblib
pref_tab_name = None
for cand in ["xgb","xgboost","rf","logistic"]:
    p = f"artifacts/{cand}.joblib"
    if os.path.exists(p):
        try:
            mdl = joblib.load(p)
            pref_tab_name = cand
            model = mdl
            break
        except Exception as e:
            continue

if pref_tab_name is None:
    raise RuntimeError("No baseline model found in artifacts/. Run baseline training first.")

print("Using baseline model:", pref_tab_name)
expected = getattr(model, "n_features_in_", None)
print("Baseline expects feature count:", expected)

# Build candidate numeric columns with non-null counts
numeric_info = []
for c in df_masked.columns:
    # try coerce
    ser = pd.to_numeric(df_masked[c].astype(str).str.replace(',',''), errors='coerce')
    nonnull = ser.notna().sum()
    unique = ser.nunique(dropna=True)
    numeric_info.append((c, nonnull, unique))

# rank by non-null descending, prefer higher unique values
numeric_info_sorted = sorted(numeric_info, key=lambda x: (x[1], x[2]), reverse=True)
# show top 20 candidates
print("Top numeric column candidates (name, non-null count, unique):")
for row in numeric_info_sorted[:30]:
    print(row)

# Select top-K numeric columns to match expected
if expected is None:
    # fallback: choose first 39 (safe)
    K = min(39, len(numeric_info_sorted))
else:
    K = min(expected, len(numeric_info_sorted))

selected_cols = [r[0] for r in numeric_info_sorted[:K]]
print("\nSelected columns ({}):".format(len(selected_cols)))
print(selected_cols)

# Build X_tab_full
X_tab_full = df_masked[selected_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values
print("Constructed feature matrix shape:", X_tab_full.shape)

# Recreate y and split (same random_state)
le = None
# build y label vector
y_raw = df_masked['final_label'].astype(str).values
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y_raw)

# train/test split
X_train_tab, X_test_tab, y_train, y_test = train_test_split(
    X_tab_full, y, stratify=y, test_size=0.2, random_state=42
)

# apply saved scaler if exists
scaler_path = "artifacts/scaler.joblib"
if os.path.exists(scaler_path):
    scaler = joblib.load(scaler_path)
    try:
        X_test_tab_used = scaler.transform(X_test_tab)
        print("Applied saved scaler to test features.")
    except Exception as e:
        print("Saved scaler failed to transform (shape may mismatch). Using StandardScaler fallback.")
        from sklearn.preprocessing import StandardScaler
        scaler2 = StandardScaler().fit(X_train_tab)
        X_test_tab_used = scaler2.transform(X_test_tab)
else:
    from sklearn.preprocessing import StandardScaler
    scaler2 = StandardScaler().fit(X_train_tab)
    X_test_tab_used = scaler2.transform(X_test_tab)
    print("No saved scaler found; used freshly fitted StandardScaler.")

# Final sanity check: feature count matches expected
print("X_test_tab_used shape:", X_test_tab_used.shape)
if expected is not None and X_test_tab_used.shape[1] != expected:
    print("Warning: constructed feature dim", X_test_tab_used.shape[1], "does not equal expected", expected)
else:
    print("Feature shape matches expected.")

# Plot confusion using model.predict on X_test_tab_used
y_pred = model.predict(X_test_tab_used)
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)
ax.set_title(f"Confusion: {pref_tab_name}")
plt.tight_layout()
os.makedirs("artifacts/plots", exist_ok=True)
fig.savefig("artifacts/plots/confusion_tabular.png", dpi=200)
plt.close(fig)
print("Saved artifacts/plots/confusion_tabular.png")

# --- Confusion matrix for hybrid model ---
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load hybrid model
hybrid = joblib.load("artifacts/hybrid_final_mlp.joblib")

# Use same test set with embeddings
y_pred_hybrid = hybrid.predict(X_test)
cm_hybrid = confusion_matrix(y_test, y_pred_hybrid)

disp = ConfusionMatrixDisplay(confusion_matrix=cm_hybrid, display_labels=le.classes_)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, cmap=plt.cm.Oranges, colorbar=False)
ax.set_title("Confusion: Hybrid (Tabular + Embeddings)")
plt.tight_layout()
os.makedirs("artifacts/plots", exist_ok=True)
fig.savefig("artifacts/plots/confusion_hybrid.png", dpi=200)
plt.close(fig)

print("✅ Saved artifacts/plots/confusion_hybrid.png")

import json, os

# Load metrics from earlier
with open("artifacts/metrics_baselines.json") as f:
    baseline_metrics = json.load(f)

hybrid_metrics = {
    "accuracy": float(0.9151),
    "f1_macro": float(0.7393),
    "note": "Hybrid struggles on Ponzi (few samples), better structural capture for Rug Pull."
}

# Save hybrid metrics
with open("artifacts/metrics_hybrid.json", "w") as f:
    json.dump(hybrid_metrics, f, indent=2)

# Markdown summary
md = f"""# Day 2 Results — Baselines & Hybrid

## Baselines
- Logistic, RF, XGB trained on tabular features.
- Best performance: **XGB** with macro F1 ≈ {baseline_metrics['xgb']['f1_macro']:.3f}.

## Hybrid (Tabular + Graph Embeddings)
- Accuracy: {hybrid_metrics['accuracy']:.3f}
- Macro F1: {hybrid_metrics['f1_macro']:.3f}
- Observations: {hybrid_metrics['note']}

## Confusion Matrices
- Tabular: `artifacts/plots/confusion_tabular.png`
- Hybrid: `artifacts/plots/confusion_hybrid.png`

---

**Takeaway:** Baselines (XGB) are very strong due to clean tabular features.
Hybrid adds novelty with graph embeddings but needs more balanced data to shine.
"""

os.makedirs("docs", exist_ok=True)
with open("docs/day2_results.md", "w") as f:
    f.write(md)

print("✅ Wrote docs/day2_results.md and artifacts/metrics_hybrid.json")

# Colab cell: Quick SMOTE + Class-weight experiment + Ablation (Tabular / Embeddings / Hybrid)
!pip install -q imbalanced-learn

import os, json, joblib, numpy as np, pandas as pd
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

os.makedirs("artifacts/plots", exist_ok=True)

# --- helper: load merged df used in your pipeline ---
labels_fp = "labels_final_patched_autolabeled.csv"
orig_fp = "transaction_dataset.csv"

if not os.path.exists(labels_fp) or not os.path.exists(orig_fp):
    raise FileNotFoundError("Make sure labels_final_patched_autolabeled.csv and transaction_dataset.csv exist in cwd.")

labels = pd.read_csv(labels_fp, dtype=str)
orig = pd.read_csv(orig_fp, low_memory=False, dtype=str)

# normalize col names
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
if 'address' not in orig.columns:
    orig = orig.rename(columns={orig.columns[0]:'address'})

df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# candidate tabular features used previously (fallback to numeric detection)
candidate_cols = ['Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
                  'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses']
feat_cols = [c for c in df.columns if c in candidate_cols]
if len(feat_cols) < 4:
    tmp = df.copy()
    for c in tmp.columns:
        tmp[c] = pd.to_numeric(tmp[c].astype(str).str.replace(',',''), errors='coerce')
    feat_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()[:8]

print("Tabular features chosen:", feat_cols)

# load embeddings if present (SVD / node2vec / graph embeddings options)
emb = None
for candidate in ["artifacts/structural_embeddings_svd.npy",
                  "artifacts/structural_embeddings_node2vec.npy",
                  "artifacts/graph_embeddings.npy"]:
    if os.path.exists(candidate):
        emb = np.load(candidate)
        print("Loaded embeddings from:", candidate, "shape:", emb.shape)
        break

# build feature matrices
X_tab = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values
if emb is not None:
    # ensure emb aligns with df rows (previous code produced aligned emb)
    if emb.shape[0] != len(df):
        raise RuntimeError(f"Embedding rows ({emb.shape[0]}) != df rows ({len(df)}). Ensure node_map alignment.")
    X_emb = emb
else:
    X_emb = None

# labels: filter to classes with at least N_min examples to keep experiments stable
y_raw = df['final_label'].astype(str).values
label_counts = Counter(y_raw)
N_min = 5   # keep classes with >= N_min examples
valid_labels = [lbl for lbl,cnt in label_counts.items() if cnt >= N_min]
mask = df['final_label'].astype(str).isin(valid_labels)
print("Valid labels (kept):", valid_labels)

X_tab = X_tab[mask.values]
y_raw = df['final_label'].astype(str)[mask.values].values
if X_emb is not None:
    X_emb = X_emb[mask.values]

# encode labels
le = LabelEncoder()
y = le.fit_transform(y_raw)
classes = le.classes_.tolist()
print("Classes used:", classes)

# train/test split (deterministic)
Xt_tab_train, Xt_tab_test, y_train, y_test = train_test_split(X_tab, y, stratify=y, test_size=0.2, random_state=42)
if X_emb is not None:
    Xt_emb_train, Xt_emb_test, _, _ = train_test_split(X_emb, y, stratify=y, test_size=0.2, random_state=42)
else:
    Xt_emb_train = Xt_emb_test = None

# helper: choose model (XGBoost if available else RF)
def get_tabular_model():
    try:
        import xgboost as xgb
        model = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, n_jobs=4, random_state=42)
        print("Using XGBoost for experiments.")
    except Exception:
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
        print("XGBoost not available — using RandomForest.")
    return model

# training + eval wrapper
def train_and_eval(model, X_train, X_test, y_train, y_test, scale=True, name="model"):
    # scale
    if scale:
        scaler = StandardScaler().fit(X_train)
        X_train_s = scaler.transform(X_train)
        X_test_s = scaler.transform(X_test)
    else:
        X_train_s, X_test_s = X_train, X_test
    # fit
    model.fit(X_train_s, y_train)
    y_pred = model.predict(X_test_s)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)
    per_prec, per_rec, per_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)
    return {
        "name": name,
        "acc": float(acc),
        "f1_macro": float(f1),
        "per_class_precision": dict(zip(le.classes_, per_prec.tolist())),
        "per_class_recall": dict(zip(le.classes_, per_rec.tolist())),
        "report": report
    }

# compute class weights for loss/sampling
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
# map to model expected order (we'll pass weights or use sample resampling)
print("Computed class weights (by encoded class index):", class_weights.tolist())

results = {}

# ============== Experiment 1: Tabular-only (with SMOTE) =================
print("\n--- Experiment: Tabular-only with SMOTE ---")
sm = SMOTE(random_state=42)
X_tab_res, y_res = sm.fit_resample(Xt_tab_train, y_train)
print("SMOTE resampled shape:", X_tab_res.shape)
tab_model = get_tabular_model()
metrics_tab = train_and_eval(tab_model, X_tab_res, Xt_tab_test, y_res, y_test, scale=True, name="tabular_smote")
results['tabular_smote'] = metrics_tab
joblib.dump(tab_model, "artifacts/tabular_smote_model.joblib")
print("Tabular-only F1_macro:", metrics_tab['f1_macro'])

# ============== Experiment 2: Embedding-only (no SMOTE usually) =================
if Xt_emb_train is not None:
    print("\n--- Experiment: Embedding-only (no SMOTE) ---")
    emb_model = get_tabular_model()
    metrics_emb = train_and_eval(emb_model, Xt_emb_train, Xt_emb_test, y_train, y_test, scale=True, name="embed_only")
    results['embed_only'] = metrics_emb
    joblib.dump(emb_model, "artifacts/embed_only_model.joblib")
    print("Embedding-only F1_macro:", metrics_emb['f1_macro'])
else:
    print("\nNo embeddings found; skipping embedding-only experiment.")

# ============== Experiment 3: Hybrid (Tabular + Embedding) =================
if Xt_emb_train is not None:
    print("\n--- Experiment: Hybrid (concat) with SMOTE applied to combined space ---")
    # concat
    X_train_h = np.hstack([Xt_tab_train, Xt_emb_train])
    X_test_h  = np.hstack([Xt_tab_test, Xt_emb_test])
    # SMOTE on combined space (ok for experiment; or you can SMOTE only tabular part separately)
    sm_h = SMOTE(random_state=42)
    X_h_res, y_h_res = sm_h.fit_resample(X_train_h, y_train)
    print("Hybrid SMOTE resampled shape:", X_h_res.shape)
    hybrid_model = get_tabular_model()
    metrics_hybrid = train_and_eval(hybrid_model, X_h_res, X_test_h, y_h_res, y_test, scale=True, name="hybrid_smote")
    results['hybrid_smote'] = metrics_hybrid
    joblib.dump(hybrid_model, "artifacts/hybrid_smote_model.joblib")
    print("Hybrid F1_macro:", metrics_hybrid['f1_macro'])
else:
    print("\nEmbeddings missing; skipping hybrid experiment.")

# ============== Save metrics & quick comparison plot =================
with open("artifacts/ablation_metrics.json", "w") as f:
    json.dump(results, f, indent=2)
print("\nSaved artifacts/ablation_metrics.json")

# simple bar plot of macro-F1 for each run
names = []
f1s = []
for k,v in results.items():
    names.append(k)
    f1s.append(v['f1_macro'])

plt.figure(figsize=(6,4))
plt.bar(names, f1s)
plt.ylim(0,1)
plt.ylabel("Macro F1")
plt.title("Ablation: Macro F1 (higher is better)")
plt.xticks(rotation=20)
plt.tight_layout()
plt.savefig("artifacts/plots/ablation_f1.png", dpi=200)
plt.close()
print("Saved artifacts/plots/ablation_f1.png")

# print summary table
print("\n=== Summary ===")
for k,v in results.items():
    print(f"{k}: acc={v['acc']:.3f}, f1_macro={v['f1_macro']:.3f}")

# show per-class recalls for hybrid (if present)
if 'hybrid_smote' in results:
    print("\nPer-class recall (hybrid):")
    for cls,rec in results['hybrid_smote']['per_class_recall'].items():
        print(f"  {cls}: {rec:.2f}")

print("\nDone — artifacts saved under artifacts/. If you want, I can now: (1) add class-weighted PyTorch MLP training, (2) focal-loss option, or (3) run a hyperparam sweep. Which next?")

# Colab cell: PyTorch class-weighted MLP + optional focal loss (Tabular & Hybrid)
!pip install -q torch torchvision torchaudio

import os, json, joblib, numpy as np, pandas as pd, time
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

os.makedirs("artifacts/plots", exist_ok=True)
os.makedirs("artifacts/pytorch", exist_ok=True)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

# ---- Load same merged df as before ----
labels_fp = "labels_final_patched_autolabeled.csv"
orig_fp = "transaction_dataset.csv"
if not os.path.exists(labels_fp) or not os.path.exists(orig_fp):
    raise FileNotFoundError("Ensure labels_final_patched_autolabeled.csv and transaction_dataset.csv are in the working dir.")

labels = pd.read_csv(labels_fp, dtype=str)
orig = pd.read_csv(orig_fp, low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
if 'address' not in orig.columns:
    orig = orig.rename(columns={orig.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

# Tabular features (same as your chosen set)
candidate_cols = ['Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
                  'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses']
feat_cols = [c for c in df.columns if c in candidate_cols]
if len(feat_cols) < 4:
    tmp = df.copy()
    for c in tmp.columns:
        tmp[c] = pd.to_numeric(tmp[c].astype(str).str.replace(',',''), errors='coerce')
    feat_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()[:8]
print("Tabular features:", feat_cols)

# Load best structural embeddings available (SVD / node2vec / graph)
emb = None
for candidate in ["artifacts/structural_embeddings_svd.npy",
                  "artifacts/structural_embeddings_node2vec.npy",
                  "artifacts/graph_embeddings.npy"]:
    if os.path.exists(candidate):
        emb = np.load(candidate)
        print("Loaded embeddings from:", candidate, "shape:", emb.shape)
        break

# Filter labels to stable classes (same threshold as before)
y_raw = df['final_label'].astype(str).values
label_counts = Counter(y_raw)
N_min = 5
valid_labels = [lbl for lbl,cnt in label_counts.items() if cnt >= N_min]
mask = df['final_label'].astype(str).isin(valid_labels)
print("Valid labels kept:", valid_labels)

X_tab = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).values[mask.values]
y_keep = df['final_label'].astype(str)[mask.values].values
if emb is not None:
    emb = emb[mask.values]
else:
    print("No embeddings found; hybrid experiments will be skipped.")

# Encode labels
le = LabelEncoder()
y = le.fit_transform(y_keep)
classes = le.classes_.tolist()
print("Classes order:", classes)

# Train/test split
X_tab_train, X_tab_test, y_train, y_test = train_test_split(X_tab, y, stratify=y, test_size=0.2, random_state=42)
if emb is not None:
    X_emb_train, X_emb_test, _, _ = train_test_split(emb, y, stratify=y, test_size=0.2, random_state=42)
else:
    X_emb_train = X_emb_test = None

# Standardize (fit on train)
scaler_tab = StandardScaler().fit(X_tab_train)
X_tab_train_s = scaler_tab.transform(X_tab_train)
X_tab_test_s  = scaler_tab.transform(X_tab_test)
joblib.dump(scaler_tab, "artifacts/pytorch/tabular_scaler.joblib")

# Hybrid matrices if emb exists
if X_emb_train is not None:
    X_h_train = np.hstack([X_tab_train_s, X_emb_train])
    X_h_test  = np.hstack([X_tab_test_s,  X_emb_test])
else:
    X_h_train = X_tab_train_s
    X_h_test  = X_tab_test_s

# Device tensors helper
def to_tensor(x, dtype=torch.float32):
    return torch.tensor(x, dtype=dtype)

# Compute class weights (inverse freq) for loss
counts = np.bincount(y_train)
# avoid zeros
counts = np.where(counts==0, 1, counts)
class_weights = (len(y_train) / (len(counts) * counts)).astype(float)
print("Class weights (for encoded classes):", class_weights.tolist())
weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)

# Focal loss implementation
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        if alpha is None:
            self.alpha = None
        else:
            self.alpha = torch.tensor(alpha, dtype=torch.float32).to(DEVICE)
        self.reduction = reduction

    def forward(self, inputs, targets):
        # inputs: logits (batch, C)
        ce = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        p_t = torch.exp(-ce)
        loss = ((1 - p_t) ** self.gamma) * ce
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

# Simple MLP model
class MLP(nn.Module):
    def __init__(self, in_dim, hidden=(256,128), num_classes=4, dropout=0.3):
        super().__init__()
        layers = []
        prev = in_dim
        for h in hidden:
            layers.append(nn.Linear(prev, h))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev = h
        layers.append(nn.Linear(prev, num_classes))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# training helper
def train_model(model, loss_fn, opt, train_loader, val_X, val_y, epochs=40, name="model"):
    best = {"f1": -1, "metrics": None}
    for epoch in range(1, epochs+1):
        model.train()
        total_loss = 0.0
        for xb, yb in train_loader:
            xb = xb.to(DEVICE); yb = yb.to(DEVICE)
            logits = model(xb)
            loss = loss_fn(logits, yb)
            opt.zero_grad(); loss.backward(); opt.step()
            total_loss += loss.item() * xb.size(0)
        # val
        model.eval()
        with torch.no_grad():
            Xv = torch.tensor(val_X, dtype=torch.float32).to(DEVICE)
            logits = model(Xv)
            preds = logits.argmax(dim=1).cpu().numpy()
        acc = accuracy_score(val_y, preds)
        f1 = f1_score(val_y, preds, average='macro')
        if f1 > best["f1"]:
            best["f1"] = f1
            best["metrics"] = {"epoch": epoch, "acc": acc, "f1": f1, "preds": preds}
            # save snapshot
            torch.save(model.state_dict(), f"artifacts/pytorch/{name}_best.pth")
        if epoch % 5 == 0 or epoch==1:
            print(f"{name} epoch {epoch} loss={total_loss/len(train_loader.dataset):.4f} val_acc={acc:.4f} val_f1={f1:.4f}")
    # load best
    model.load_state_dict(torch.load(f"artifacts/pytorch/{name}_best.pth"))
    return best

# Build dataloaders and train 3 models: tabular-only, hybrid weighted CE, hybrid focal
batch_size = 64
num_classes = len(classes)

metrics_out = {}

# ---------- Tabular-only (class-weighted CE) ----------
print("\nTraining tabular-only model (class-weighted CE)...")
X_train_t = X_tab_train_s
X_test_t  = X_tab_test_s
train_ds_t = TensorDataset(to_tensor(X_train_t), torch.tensor(y_train, dtype=torch.long))
train_loader_t = DataLoader(train_ds_t, batch_size=batch_size, shuffle=True, drop_last=False)
model_tab = MLP(in_dim=X_train_t.shape[1], hidden=(256,128), num_classes=num_classes).to(DEVICE)
loss_fn_tab = nn.CrossEntropyLoss(weight=weights_tensor)
opt_tab = torch.optim.Adam(model_tab.parameters(), lr=1e-3, weight_decay=1e-5)
best_tab = train_model(model_tab, loss_fn_tab, opt_tab, train_loader_t, X_test_t, y_test, epochs=40, name="tabular")
# final eval
model_tab.eval()
with torch.no_grad():
    logits = model_tab(torch.tensor(X_test_t, dtype=torch.float32).to(DEVICE))
    preds = logits.argmax(dim=1).cpu().numpy()
acc = accuracy_score(y_test, preds); f1m = f1_score(y_test, preds, average='macro')
prec, rec, f1s, sup = precision_recall_fscore_support(y_test, preds, average=None, zero_division=0)
metrics_out['tabular_pytorch'] = {"acc": float(acc), "f1_macro": float(f1m),
                                 "per_class_precision": dict(zip(classes, prec.tolist())),
                                 "per_class_recall": dict(zip(classes, rec.tolist()))}
torch.save(model_tab.state_dict(), "artifacts/pytorch/tabular_final.pth")
print("Tabular done:", metrics_out['tabular_pytorch'])

# ---------- Hybrid (concat) with class-weighted CE ----------
if X_emb_train is not None:
    print("\nTraining hybrid model (class-weighted CE)...")
    X_train_h = np.hstack([X_tab_train_s, X_emb_train])
    X_test_h  = np.hstack([X_tab_test_s,  X_emb_test])
    train_ds_h = TensorDataset(to_tensor(X_train_h), torch.tensor(y_train, dtype=torch.long))
    train_loader_h = DataLoader(train_ds_h, batch_size=batch_size, shuffle=True)
    model_h = MLP(in_dim=X_train_h.shape[1], hidden=(256,128), num_classes=num_classes).to(DEVICE)
    loss_fn_h = nn.CrossEntropyLoss(weight=weights_tensor)
    opt_h = torch.optim.Adam(model_h.parameters(), lr=1e-3, weight_decay=1e-5)
    best_h = train_model(model_h, loss_fn_h, opt_h, train_loader_h, X_test_h, y_test, epochs=45, name="hybrid_ce")
    # eval
    model_h.eval()
    with torch.no_grad():
        logits = model_h(torch.tensor(X_test_h, dtype=torch.float32).to(DEVICE))
        preds = logits.argmax(dim=1).cpu().numpy()
    acc = accuracy_score(y_test, preds); f1m = f1_score(y_test, preds, average='macro')
    prec, rec, f1s, sup = precision_recall_fscore_support(y_test, preds, average=None, zero_division=0)
    metrics_out['hybrid_pytorch_ce'] = {"acc": float(acc), "f1_macro": float(f1m),
                                       "per_class_precision": dict(zip(classes, prec.tolist())),
                                       "per_class_recall": dict(zip(classes, rec.tolist()))}
    torch.save(model_h.state_dict(), "artifacts/pytorch/hybrid_ce_final.pth")
    print("Hybrid CE done:", metrics_out['hybrid_pytorch_ce'])
else:
    print("Skipping hybrid CE: no embeddings present.")

# ---------- Hybrid + Focal Loss ----------
if X_emb_train is not None:
    print("\nTraining hybrid model with Focal Loss (alpha=class_weights)...")
    # use same architecture
    model_hf = MLP(in_dim=X_train_h.shape[1], hidden=(256,128), num_classes=num_classes).to(DEVICE)
    # pass alpha as class weights so focal respects class balance
    loss_fn_hf = FocalLoss(alpha=class_weights.tolist(), gamma=2.0)
    opt_hf = torch.optim.Adam(model_hf.parameters(), lr=1e-3, weight_decay=1e-5)
    train_ds_hf = TensorDataset(to_tensor(X_train_h), torch.tensor(y_train, dtype=torch.long))
    train_loader_hf = DataLoader(train_ds_hf, batch_size=batch_size, shuffle=True)
    best_hf = train_model(model_hf, loss_fn_hf, opt_hf, train_loader_hf, X_test_h, y_test, epochs=45, name="hybrid_focal")
    # eval
    model_hf.eval()
    with torch.no_grad():
        logits = model_hf(torch.tensor(X_test_h, dtype=torch.float32).to(DEVICE))
        preds = logits.argmax(dim=1).cpu().numpy()
    acc = accuracy_score(y_test, preds); f1m = f1_score(y_test, preds, average='macro')
    prec, rec, f1s, sup = precision_recall_fscore_support(y_test, preds, average=None, zero_division=0)
    metrics_out['hybrid_pytorch_focal'] = {"acc": float(acc), "f1_macro": float(f1m),
                                          "per_class_precision": dict(zip(classes, prec.tolist())),
                                          "per_class_recall": dict(zip(classes, rec.tolist()))}
    torch.save(model_hf.state_dict(), "artifacts/pytorch/hybrid_focal_final.pth")
    print("Hybrid Focal done:", metrics_out['hybrid_pytorch_focal'])
else:
    print("Skipping hybrid focal: no embeddings present.")

# Save metrics JSON
with open("artifacts/metrics_pytorch.json", "w") as f:
    json.dump(metrics_out, f, indent=2)

print("\nSaved artifacts/metrics_pytorch.json and models under artifacts/pytorch/.")
print("Results summary:")
print(json.dumps(metrics_out, indent=2))

# Colab cell: Plot per-class precision/recall (CE vs Focal) + update docs/day2_results.md
import os, json, matplotlib.pyplot as plt, pandas as pd

metrics_path = "artifacts/metrics_pytorch.json"
assert os.path.exists(metrics_path), "Run PyTorch training first to produce metrics_pytorch.json"

with open(metrics_path) as f:
    metrics = json.load(f)

# Extract per-class metrics for CE vs Focal
def extract_perclass(mdict, key):
    prec = mdict[key]["per_class_precision"]
    rec = mdict[key]["per_class_recall"]
    return pd.DataFrame({"precision": prec, "recall": rec})

df_ce = extract_perclass(metrics, "hybrid_pytorch_ce")
df_focal = extract_perclass(metrics, "hybrid_pytorch_focal")

# Plot precision/recall side-by-side
os.makedirs("artifacts/plots", exist_ok=True)
fig, axes = plt.subplots(1,2, figsize=(10,4), sharey=True)

df_ce.plot(kind="bar", ax=axes[0])
axes[0].set_title("Hybrid CE (class-weighted)")
axes[0].set_ylim(0,1)
axes[0].legend(loc="lower right", fontsize="small")

df_focal.plot(kind="bar", ax=axes[1])
axes[1].set_title("Hybrid Focal Loss")
axes[1].set_ylim(0,1)
axes[1].legend(loc="lower right", fontsize="small")

plt.suptitle("Per-class Precision & Recall")
plt.tight_layout(rect=[0,0,1,0.95])
plt.savefig("artifacts/plots/per_class_pr_bars.png", dpi=200)
plt.close()
print("✅ Saved artifacts/plots/per_class_pr_bars.png")

# ---- Create comparison table for docs ----
rows = []
for name, m in [("Tabular CE", "tabular_pytorch"),
                ("Hybrid CE", "hybrid_pytorch_ce"),
                ("Hybrid Focal", "hybrid_pytorch_focal")]:
    rows.append({
        "Model": name,
        "Accuracy": metrics[m]["acc"],
        "Macro F1": metrics[m]["f1_macro"],
        "Ponzi Recall": metrics[m]["per_class_recall"]["ponzi"]
    })
df_table = pd.DataFrame(rows)

# Round values for markdown
df_table_rounded = df_table.copy()
df_table_rounded[["Accuracy","Macro F1","Ponzi Recall"]] = df_table_rounded[["Accuracy","Macro F1","Ponzi Recall"]].round(3)

# Save as markdown table
md_table = df_table_rounded.to_markdown(index=False)

# Append/update docs/day2_results.md
doc_path = "docs/day2_results.md"
with open(doc_path, "a") as f:
    f.write("\n\n## Extra: PyTorch CE vs Focal Comparison\n")
    f.write(md_table + "\n")
    f.write("\n![Per-class Precision/Recall](../artifacts/plots/per_class_pr_bars.png)\n")

print("✅ Updated docs/day2_results.md with CE vs Focal table and plot")
print("\n=== Comparison Table ===\n")
print(md_table)

import zipfile, os
from google.colab import files

include_paths = [
    "artifacts", "docs", "sample_data",
    "DATASET.md", "alchemy_summary.csv",
    "labels_candidates.csv", "labels_candidates_with_cluster.csv",
    "labels_final.csv", "labels_final_patched.csv", "labels_final_patched_autolabeled.csv",
    "labels_final_with_notes.csv", "labels_for_annotation.xlsx", "labels_for_annotation_enriched.xlsx",
    "to_review_for_alchemy.xlsx", "to_review_for_alchemy_auto.csv", "transaction_dataset.csv"
]

zip_name = "capstone_files.zip"
with zipfile.ZipFile(zip_name, 'w') as zipf:
    for path in include_paths:
        if os.path.exists(path):
            if os.path.isdir(path):
                # add directory contents
                for root, dirs, files_in_dir in os.walk(path):
                    for file in files_in_dir:
                        file_path = os.path.join(root, file)
                        zipf.write(file_path)
            else:
                zipf.write(path)
        else:
            print("Skipping missing:", path)

# Download zip
files.download(zip_name)

# Colab cell: Manual FedAvg federated simulation using sklearn only (no Flower)
# Paste & run in your current Colab runtime (no installs required)
import os, json, random, time
from collections import Counter
import numpy as np, pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import joblib
import matplotlib.pyplot as plt

# ---- CONFIG ----
LABELS_FP = "labels_final_patched_autolabeled.csv"
TRANSACT_FP = "transaction_dataset.csv"
EMB_FP = "artifacts/structural_embeddings_svd.npy"   # optional
ARTIFACTS_DIR = "artifacts"
os.makedirs(ARTIFACTS_DIR, exist_ok=True)
os.makedirs("artifacts/plots", exist_ok=True)

N_CLIENTS = 4
ROUNDS = 10
NON_IID = True   # True => label-skew shards; False => random IID shards
SEED = 42

random.seed(SEED); np.random.seed(SEED)

# ---- Load data & build features (same as your hybrid) ----
print("Loading data...")
labels = pd.read_csv(LABELS_FP, dtype=str)
orig = pd.read_csv(TRANSACT_FP, low_memory=False, dtype=str)
if 'Address' in orig.columns:
    orig = orig.rename(columns={'Address':'address'})
if 'address' not in labels.columns:
    labels = labels.rename(columns={labels.columns[0]:'address'})
df = labels.merge(orig, on='address', how='left').reset_index(drop=True)

tab_cols = ['Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
            'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses']
for c in tab_cols:
    df[c] = pd.to_numeric(df.get(c, 0), errors='coerce').fillna(0.0)

# embeddings optional
if os.path.exists(EMB_FP):
    emb = np.load(EMB_FP)
    if emb.shape[0] != len(df):
        raise RuntimeError("Embeddings rows != df rows; align your node map.")
    X_full = np.hstack([df[tab_cols].values, emb])
    print("Using hybrid features:", X_full.shape)
else:
    X_full = df[tab_cols].values
    print("Using tabular-only features:", X_full.shape)

y_raw = df['final_label'].astype(str).values
counts = Counter(y_raw)
valid_labels = [lbl for lbl,cnt in counts.items() if cnt >= 5]
mask = np.isin(y_raw, valid_labels)
X_full = X_full[mask]
y_raw = y_raw[mask]
print("Kept rows:", X_full.shape[0], "classes:", valid_labels)

le = LabelEncoder()
y_enc = le.fit_transform(y_raw)
print("Classes:", le.classes_)

# Train/test holdout
X_train, X_test, y_train, y_test = train_test_split(X_full, y_enc, test_size=0.2, stratify=y_enc, random_state=SEED)

# Scale
scaler = StandardScaler().fit(X_train)
X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
joblib.dump(scaler, os.path.join(ARTIFACTS_DIR, "fl_manual_scaler.joblib"))

# ---- create shards ----
def create_shards(X, y, n_clients=N_CLIENTS, non_iid=NON_IID):
    idxs = np.arange(len(X))
    if not non_iid:
        np.random.shuffle(idxs)
        shards = np.array_split(idxs, n_clients)
    else:
        # label-skew: give each client a majority of one/few labels
        shards = [[] for _ in range(n_clients)]
        for cls in np.unique(y):
            cls_idxs = idxs[y==cls]
            np.random.shuffle(cls_idxs)
            chunks = np.array_split(cls_idxs, n_clients)
            for i, chunk in enumerate(chunks):
                shards[i].extend(chunk.tolist())
        shards = [np.array(s, dtype=int) for s in shards]
    return [(X[s], y[s]) for s in shards]

shards = create_shards(X_train_s, y_train, N_CLIENTS, NON_IID)
for i,(Xi, yi) in enumerate(shards):
    print(f"Client {i}: {Xi.shape[0]} samples | label dist: {Counter(yi)}")

# ---- initialize global model params ----
n_features = X_train_s.shape[1]
n_classes = len(np.unique(y_train))
# We'll use multinomial logistic regression; initialize global coef & intercept as zeros
global_coef = np.zeros((n_classes, n_features), dtype=np.float64)
global_intercept = np.zeros((n_classes,), dtype=np.float64)

# Helper: fit local client model starting from global params (warm start)
def fit_local(X_local, y_local, init_coef=None, init_intercept=None):
    # build LogisticRegression and set coef_ if provided, then fit (warm_start)
    model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=500, warm_start=True)
    if init_coef is not None:
        # assign attributes to mimic a fitted model so warm_start continues from there
        model.coef_ = np.array(init_coef)
        model.intercept_ = np.array(init_intercept)
        model.classes_ = np.arange(n_classes)
    # fit on local data
    try:
        model.fit(X_local, y_local)
    except Exception:
        # fallback to lbfgs solver
        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=300)
        model.fit(X_local, y_local)
    return model.coef_.astype(np.float64), model.intercept_.astype(np.float64), len(X_local)

# ---- Federated rounds (manual FedAvg) ----
history = {"rounds": []}
for rnd in range(1, ROUNDS+1):
    # Each client fits locally and returns params
    client_params = []
    client_sizes = []
    for i,(Xi, yi) in enumerate(shards):
        # client trains starting from global params
        coef_i, int_i, size_i = fit_local(Xi, yi, init_coef=global_coef, init_intercept=global_intercept)
        client_params.append((coef_i, int_i))
        client_sizes.append(size_i)
    total_samples = sum(client_sizes)
    # aggregate weighted average
    new_coef = np.zeros_like(global_coef)
    new_intercept = np.zeros_like(global_intercept)
    for (coef_i, int_i), sz in zip(client_params, client_sizes):
        w = sz / total_samples
        new_coef += coef_i * w
        new_intercept += int_i * w
    global_coef = new_coef
    global_intercept = new_intercept

    # Evaluate aggregated global model on shared holdout
    # create a LogisticRegression-like object for prediction using aggregated params
    class GlobalModel:
        def __init__(self, coef, intercept):
            self.coef_ = np.array(coef)
            self.intercept_ = np.array(intercept)
            self.classes_ = np.arange(len(self.intercept_))
        def predict(self, X):
            logits = X.dot(self.coef_.T) + self.intercept_
            return np.argmax(logits, axis=1)

    gmodel = GlobalModel(global_coef, global_intercept)
    preds = gmodel.predict(X_test_s)
    acc = accuracy_score(y_test, preds)
    f1m = f1_score(y_test, preds, average='macro')
    print(f"Round {rnd}: global_acc={acc:.4f}, global_f1={f1m:.4f}")
    history["rounds"].append({"round": rnd, "acc": float(acc), "f1_macro": float(f1m)})

# Save history & final global model
with open(os.path.join(ARTIFACTS_DIR, "fl_manual_history.json"), "w") as f:
    json.dump(history, f, indent=2)
np.save(os.path.join(ARTIFACTS_DIR, "fl_manual_global_coef.npy"), global_coef)
np.save(os.path.join(ARTIFACTS_DIR, "fl_manual_global_intercept.npy"), global_intercept)
print("Saved artifacts/fl_manual_history.json and global model params.")

# ---- Centralized baseline for comparison ----
central = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000)
central.fit(X_train_s, y_train)
preds_c = central.predict(X_test_s)
central_acc = accuracy_score(y_test, preds_c)
central_f1 = f1_score(y_test, preds_c, average='macro')
print("Centralized test acc:", central_acc, "f1_macro:", central_f1)

# Save comparison and plot
comparison = {"centralized": {"acc": central_acc, "f1_macro": central_f1}, "federated": history}
with open(os.path.join(ARTIFACTS_DIR, "fl_manual_vs_central.json"), "w") as f:
    json.dump(comparison, f, indent=2)

# plot rounds vs acc
rounds = [r["round"] for r in history["rounds"]]
accs = [r["acc"] for r in history["rounds"]]
plt.figure(figsize=(6,4))
plt.plot(rounds, accs, marker='o', label='Federated global acc')
plt.hlines(central_acc, xmin=min(rounds), xmax=max(rounds), colors='r', linestyles='--', label='Centralized acc')
plt.xlabel("Round")
plt.ylabel("Accuracy")
plt.title("Manual FedAvg: Federated global vs Centralized")
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(ARTIFACTS_DIR, "plots/fl_manual_vs_central.png"), dpi=200)
plt.close()
print("Saved artifacts/plots/fl_manual_vs_central.png")

# Build a scikit-learn compatible LogisticRegression from saved FedAvg params
import numpy as np, joblib, os
from sklearn.linear_model import LogisticRegression

ART = "artifacts"
coef_fp = os.path.join(ART, "fl_manual_global_coef.npy")
int_fp  = os.path.join(ART, "fl_manual_global_intercept.npy")
out_fp  = os.path.join(ART, "fl_manual_global_model.joblib")

if not (os.path.exists(coef_fp) and os.path.exists(int_fp)):
    raise FileNotFoundError("Saved global params not found. Run FedAvg cell first.")

coef = np.load(coef_fp)
intercept = np.load(int_fp)

# Create a LogisticRegression object and attach params
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1)
model.coef_ = coef
model.intercept_ = intercept
# set classes_ (0..C-1)
model.classes_ = np.arange(coef.shape[0])

joblib.dump(model, out_fp)
print("Saved aggregated model to", out_fp)

# Colab cell: Create publication-quality plots for report/slide/demo
import os, json, math
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import joblib

plt.rcParams.update({
    "figure.dpi": 200,
    "font.size": 10,
    "savefig.bbox": "tight"
})

PLOT_DIR = "artifacts/plots"
os.makedirs(PLOT_DIR, exist_ok=True)
ART_DIR = "artifacts"

# Helper: load metrics files if exist
def load_json(fname):
    p = os.path.join(ART_DIR, fname)
    if os.path.exists(p):
        return json.load(open(p))
    return None

metrics_ablation = load_json("ablation_metrics.json")
metrics_pytorch  = load_json("metrics_pytorch.json")
metrics_hybrid   = load_json("metrics_hybrid.json")
fl_manual_vs_central = load_json("fl_manual_vs_central.json") or load_json("fl_manual_vs_central.json")
fl_manual_history = load_json("fl_manual_history.json") or load_json("flower_history.json")

# ---------- 1) Confusion matrices ----------
# Try to load saved predictions if available under artifacts; else compute from saved models (sklearn only)
def plot_confusion_from_saved(model_path, scaler_path, X_test, y_test, title, outname):
    try:
        model = joblib.load(model_path)
        scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None
        if scaler is not None:
            Xs = scaler.transform(X_test)
        else:
            Xs = X_test
        y_pred = model.predict(Xs)
        cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))
        disp = ConfusionMatrixDisplay(cm, display_labels=getattr(model, "classes_", np.unique(y_test)))
        fig, ax = plt.subplots(figsize=(5,4))
        disp.plot(ax=ax, cmap="Blues", colorbar=False)
        ax.set_title(title)
        plt.xticks(rotation=45)
        plt.savefig(os.path.join(PLOT_DIR, outname), dpi=200)
        plt.close()
        print("Saved", outname)
    except Exception as e:
        print("Could not plot confusion for", model_path, ":", e)

# If we've saved a centralized sklearn model and a scaler and a test split earlier, try to reload them
# Fallback: try to reconstruct test set from saved scaler and X_test.npy / y_test.npy if present
def try_load_test_arrays():
    # common names we saved earlier
    candidates = [
        ("artifacts/fl_manual_global_model.joblib","artifacts/fl_manual_global_model.joblib"),
    ]
    # try heuristics: if fl_manual_vs_central.json exists it contains centralized acc; we may not have X_test saved
    # So first try to find X_test/y_test saved in artifacts (common names)
    for xname in ["artifacts/X_test.npy","artifacts/x_test.npy","artifacts/test_X.npy"]:
        if os.path.exists(xname):
            X_test = np.load(xname)
            break
    else:
        X_test = None
    for yname in ["artifacts/y_test.npy","artifacts/test_y.npy","artifacts/y_test_enc.npy"]:
        if os.path.exists(yname):
            y_test = np.load(yname)
            break
    else:
        y_test = None
    return X_test, y_test

# Use models if available
# Try: tabular_smote model, hybrid_smote model, pytorch saved models -> but sklearn confusions easiest
if os.path.exists(os.path.join(ART_DIR, "tabular_smote_model.joblib")) and os.path.exists(os.path.join(ART_DIR, "fl_manual_scaler_sklearn.joblib")):
    # If we have X_test saved, use it; else try to load test arrays; else skip
    X_test, y_test = try_load_test_arrays()
    if X_test is None:
        print("No X_test saved; skipping automatic confusion from saved models. If you want, save test arrays to artifacts/X_test.npy and artifacts/y_test.npy and re-run.")
    else:
        plot_confusion_from_saved("artifacts/tabular_smote_model.joblib","artifacts/fl_manual_scaler_sklearn.joblib",X_test,y_test,"Tabular model (saved)","confusion_tabular_saved.png")

# If we have a confusion image already created earlier (names used in pipeline), copy to plots dir
for candidate in ["artifacts/plots/confusion_hybrid.png","artifacts/plots/confusion_tabular.png","artifacts/confusion_hybrid.png","artifacts/confusion_tabular.png"]:
    if os.path.exists(candidate):
        dest = os.path.join(PLOT_DIR, os.path.basename(candidate))
        try:
            import shutil
            shutil.copy(candidate, dest)
            print("Copied existing confusion:", dest)
        except:
            pass

# ---------- 2) Ablation Macro-F1 bar (from ablation_metrics.json) ----------
if metrics_ablation:
    names, f1s = [], []
    for k,v in metrics_ablation.items():
        names.append(k.replace("_"," ").title())
        f1s.append(v.get("f1_macro", 0.0))
    # sort by f1 desc for nicer plot
    order = np.argsort(f1s)[::-1]
    names = [names[i] for i in order]
    f1s = [f1s[i] for i in order]
    fig, ax = plt.subplots(figsize=(6,3))
    bars = ax.bar(names, f1s, color=sns.color_palette("pastel", len(names)))
    ax.set_ylim(0,1)
    ax.set_ylabel("Macro F1")
    ax.set_title("Ablation: Macro F1 by Model")
    ax.bar_label(bars, fmt="%.3f", padding=3)
    plt.xticks(rotation=25)
    plt.tight_layout()
    ablation_fp = os.path.join(PLOT_DIR, "ablation_macro_f1_pretty.png")
    plt.savefig(ablation_fp, dpi=300)
    plt.close()
    print("Saved", ablation_fp)
else:
    print("No ablation_metrics.json found — skipping ablation plot.")

# ---------- 3) Per-class Precision & Recall bars (CE vs Focal) ----------
if metrics_pytorch and "hybrid_pytorch_ce" in metrics_pytorch and "hybrid_pytorch_focal" in metrics_pytorch:
    ce = metrics_pytorch["hybrid_pytorch_ce"]
    focal = metrics_pytorch["hybrid_pytorch_focal"]
    classes = list(ce["per_class_precision"].keys())
    df_ce = pd.DataFrame({
        "precision": [ce["per_class_precision"][c] for c in classes],
        "recall": [ce["per_class_recall"][c] for c in classes]
    }, index=classes)
    df_f = pd.DataFrame({
        "precision": [focal["per_class_precision"][c] for c in classes],
        "recall": [focal["per_class_recall"][c] for c in classes]
    }, index=classes)
    # plot side-by-side subplots for CE and Focal
    fig, axes = plt.subplots(1,2, figsize=(10,4), sharey=True)
    df_ce.plot(kind="bar", ax=axes[0], rot=30)
    axes[0].set_title("Hybrid CE (class-weighted)")
    axes[0].set_ylim(0,1)
    axes[0].legend(loc="lower right")
    df_f.plot(kind="bar", ax=axes[1], rot=30)
    axes[1].set_title("Hybrid Focal Loss")
    axes[1].set_ylim(0,1)
    axes[1].legend(loc="lower right")
    plt.suptitle("Per-class Precision & Recall (CE vs Focal)")
    plt.tight_layout(rect=[0,0,1,0.95])
    pcp_fp = os.path.join(PLOT_DIR, "per_class_pr_ce_vs_focal.png")
    plt.savefig(pcp_fp, dpi=300)
    plt.close()
    print("Saved", pcp_fp)
else:
    print("metrics_pytorch missing required keys; skipping CE vs Focal per-class plot.")

# ---------- 4) Federated Learning plots ----------
# a) Global accuracy per round vs centralized
if fl_manual_history:
    # fl_manual_history might be dict with "rounds": [{"round":1,"acc":..}, ...] or mapping
    rounds = []
    accs = []
    if isinstance(fl_manual_history, dict) and "rounds" in fl_manual_history:
        for r in fl_manual_history["rounds"]:
            rounds.append(r["round"])
            accs.append(r["acc"])
    else:
        # try older format: mapping round->metrics
        try:
            for k,v in fl_manual_history.items():
                rounds.append(int(k))
                accs.append(v.get("acc", 0.0))
        except:
            pass
    # fallback to fl_manual_vs_central if present
    central_acc = None
    if fl_manual_vs_central and "centralized" in fl_manual_vs_central:
        central_acc = fl_manual_vs_central["centralized"].get("acc")
    if rounds:
        fig, ax = plt.subplots(figsize=(6,4))
        ax.plot(rounds, accs, marker="o", label="Federated global acc")
        if central_acc is not None:
            ax.hlines(central_acc, xmin=min(rounds), xmax=max(rounds), colors='r', linestyles='--', label='Centralized acc')
        ax.set_xlabel("Round")
        ax.set_ylabel("Accuracy")
        ax.set_title("Federated global accuracy per round")
        ax.legend()
        plt.tight_layout()
        fl_fp = os.path.join(PLOT_DIR, "fl_global_acc_rounds.png")
        plt.savefig(fl_fp, dpi=300)
        plt.close()
        print("Saved", fl_fp)
    else:
        print("No rounds found in FL history; skipping FL round plot.")
else:
    print("No FL history found; skipping FL plots.")

# b) Per-class recall across rounds (if available as per-round per-class)
# look for artifacts/fl_perclass_rounds.json or similar; otherwise skip
perclass_round_fp = os.path.join(ART_DIR, "fl_perclass_rounds.json")
if os.path.exists(perclass_round_fp):
    pc = load_json("fl_perclass_rounds.json")
    # pc expected format: { "rounds":[1,2,..], "per_class": {"ponzi":[...], "phishing":[...], ...} }
    rounds = pc["rounds"]
    plt.figure(figsize=(7,4))
    for cls, vals in pc["per_class"].items():
        plt.plot(rounds, vals, marker='o', label=cls)
    plt.xlabel("Round"); plt.ylabel("Recall"); plt.title("Per-class Recall over FL rounds")
    plt.legend(); plt.ylim(0,1)
    plt.savefig(os.path.join(PLOT_DIR, "fl_perclass_recall_rounds.png"), dpi=300)
    plt.close()
    print("Saved fl_perclass_recall_rounds.png")
else:
    print("No per-class FL rounds file found; skipping per-class FL plot (you can create fl_perclass_rounds.json to enable).")

# ---------- 5) Pipeline diagram (simple nice boxes + arrows) ----------
def draw_pipeline_diagram(outpath):
    fig, ax = plt.subplots(figsize=(8,3))
    ax.axis('off')
    # rectangles positions
    boxes = [
        ("Raw Kaggle\ntransactions", (0.05,0.5)),
        ("Labeling\n(heuristics + HDBSCAN\n+ Alchemy enrich)", (0.28,0.5)),
        ("Graph construction\n(Edges=txs)", (0.51,0.5)),
        ("Embeddings\n(GraphSAGE / SVD / Node2Vec)", (0.72,0.5)),
        ("Hybrid model\n(Tabular + Embeddings)", (0.50,0.15)),
        ("Federated demo\n(simulated clients)", (0.84,0.15))
    ]
    for text, (x,y) in boxes:
        ax.add_patch(plt.Rectangle((x,y-0.08), 0.22, 0.16, fill=True, color="#f3f6fb", ec="#2b6cb0"))
        ax.text(x+0.11, y, text, ha="center", va="center", fontsize=9)
    # arrows
    def arrow(a,b):
        ax.annotate("", xy=b, xytext=a, arrowprops=dict(arrowstyle="->", lw=1.6, color="#2b6cb0"))
    arrow((0.27,0.58),(0.28,0.58))
    arrow((0.5,0.58),(0.51,0.58))
    arrow((0.73,0.58),(0.72,0.58))
    arrow((0.61,0.48),(0.61,0.35))
    arrow((0.84,0.22),(0.72,0.3))
    ax.set_title("Project pipeline (high-level)")
    plt.savefig(outpath, dpi=300)
    plt.close()
    print("Saved pipeline diagram:", outpath)

draw_pipeline_diagram(os.path.join(PLOT_DIR, "pipeline_diagram.png"))

# ---------- 6) Results summary card (big numbers for slide hero) ----------
def make_summary_card(outpath):
    # best numbers heuristics
    best_tab = metrics_ablation.get("tabular_smote",{}) if metrics_ablation else {}
    best_hybrid = metrics_ablation.get("hybrid_smote",{}) if metrics_ablation else {}
    pyt = metrics_pytorch or {}
    # pick values
    tab_f1 = best_tab.get("f1_macro", None)
    hybrid_f1 = best_hybrid.get("f1_macro", None)
    hybrid_ce_f1 = pyt.get("hybrid_pytorch_ce",{}).get("f1_macro", None)
    ponzi_recall = pyt.get("hybrid_pytorch_ce",{}).get("per_class_recall",{}).get("ponzi", None)
    phishing_f1 = None
    if pyt and "hybrid_pytorch_ce" in pyt:
        phishing_f1 = pyt["hybrid_pytorch_ce"]["per_class_precision"].get("phishing", None)
    # Draw card
    fig, ax = plt.subplots(figsize=(6,2.5))
    ax.axis('off')
    ax.text(0.02,0.75, "Hybrid model (final)", fontsize=14, weight='bold')
    ax.text(0.02,0.58, f"Macro F1 ≈ {hybrid_ce_f1:.3f}" if hybrid_ce_f1 else "", fontsize=12)
    ax.text(0.02,0.40, f"Phishing F1 ≈ {phishing_f1:.3f}" if phishing_f1 else "", fontsize=12)
    ax.text(0.02,0.22, f"Ponzi recall ≈ {ponzi_recall:.2f}" if ponzi_recall else "", fontsize=12, color='#b22222')
    ax.text(0.75,0.65, "For slides:", fontsize=10)
    ax.text(0.75,0.40, "- pipeline diagram", fontsize=9)
    ax.text(0.75,0.28, "- per-class PR bars", fontsize=9)
    plt.savefig(outpath, dpi=300)
    plt.close()
    print("Saved summary card:", outpath)

make_summary_card(os.path.join(PLOT_DIR, "results_summary_card.png"))

print("\nAll done — plots saved to", PLOT_DIR)
print("Open those PNGs and tell me which ones you want retouched (colors/labels/layout) for the paper/slides/streamlit.")

"""realtime"""

from google.colab import userdata
import os

# Grab secret from Colab secrets
alchemy_key = userdata.get("ALCHEMY_API_KEY")

# Push it into environment so all our code works
os.environ["ALCHEMY_API_KEY"] = alchemy_key

print("Alchemy key set ✔️")

# Colab cell: Enrich labeled addresses with Alchemy features and save merged CSV
# Requirements: pip install -q requests tqdm
# Set your Alchemy key before running: %env ALCHEMY_API_KEY=your_key_here

import os, time, json, math
from datetime import datetime
from collections import defaultdict
import requests
import pandas as pd
import numpy as np
from tqdm import tqdm

# ---------- CONFIG ----------
ALCHEMY_KEY = os.getenv("ALCHEMY_API_KEY", None)
if not ALCHEMY_KEY:
    raise RuntimeError("Set ALCHEMY_API_KEY environment variable before running, e.g. %env ALCHEMY_API_KEY=your_key")

ALCHEMY_HTTP = os.getenv("ALCHEMY_HTTP_URL", f"https://eth-mainnet.g.alchemy.com/v2/{ALCHEMY_KEY}")
LABELS_FP = "labels_final_patched_autolabeled.csv"
TRANSACT_FP = "transaction_dataset.csv"
CACHE_FP = "artifacts/alchemy_cache.json"
OUT_CSV = "artifacts/labels_with_alchemy.csv"
# how many transfers to fetch per address (tune for rate limits / depth)
MAX_TRANSFER_COUNT = 200

os.makedirs("artifacts", exist_ok=True)

# ---------- helpers ----------
def load_cache(path):
    if os.path.exists(path):
        try:
            with open(path, "r") as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_cache(cache, path):
    with open(path, "w") as f:
        json.dump(cache, f)

def alchemy_post(method, params):
    payload = {"jsonrpc":"2.0","id":1,"method":method,"params":params}
    try:
        r = requests.post(ALCHEMY_HTTP, json=payload, timeout=20)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        print("Alchemy error:", e)
        return None

def get_transfers_for_address(address, max_count=MAX_TRANSFER_COUNT):
    """
    Uses alchemy_getAssetTransfers to fetch transfers for an address.
    Returns list of transfers or [] on failure.
    """
    # convert to checksum/lower if needed (Alchemy is flexible)
    params = [{
        "fromBlock": "0x0",
        "toAddress": address,
        "category": ["external","internal","erc20","erc721","erc1155"],
        "maxCount": hex(max_count)
    }]
    res = alchemy_post("alchemy_getAssetTransfers", params)
    if not res:
        return []
    transfers = res.get("result", {}).get("transfers", []) or []
    return transfers

def get_code(address):
    """Call eth_getCode to determine whether address is a contract (code != '0x')."""
    params = [address, "latest"]
    res = alchemy_post("eth_getCode", params)
    if not res:
        return None
    return res.get("result")

def parse_timestamp(ts):
    """Try to parse various timestamp formats into epoch seconds."""
    if ts is None:
        return None
    # Alchemy often returns ISO timestamps like '2023-05-01T12:34:56Z' inside metadata.blockTimestamp
    try:
        if isinstance(ts, str):
            if ts.endswith("Z"):
                ts = ts.replace("Z","+00:00")
            dt = datetime.fromisoformat(ts)
            return dt.timestamp()
    except Exception:
        pass
    # fallback: block number string hex? not handled here
    return None

def compute_features_from_transfers(address, transfers):
    """
    transfers: list of transfer dicts from alchemy_getAssetTransfers
    returns dict of engineered features described earlier
    """
    # default values
    feat = {
        "alchemy_tx_count": 0,
        "alchemy_sent_count": 0,
        "alchemy_recv_count": 0,
        "alchemy_unique_senders": 0,
        "alchemy_unique_receivers": 0,
        "alchemy_avg_min_between_sent": 0.0,
        "alchemy_avg_min_between_recv": 0.0,
        "alchemy_time_span_mins": 0.0,
        "alchemy_contract_interactions": 0,
        "alchemy_is_contract": 0,
        "alchemy_missing": 0
    }
    if not transfers:
        feat["alchemy_missing"] = 1
        return feat

    # normalize lower-case addresses
    addr_l = address.lower()
    sent = []
    recv = []
    contract_interactions = 0
    senders = set()
    receivers = set()
    timestamps_sent = []
    timestamps_recv = []
    timestamps_all = []

    for t in transfers:
        frm = str(t.get("from","")).lower()
        to  = str(t.get("to","")).lower()
        # incremental counts
        if frm == addr_l:
            sent.append(t)
        if to == addr_l:
            recv.append(t)
        # counterparties
        if to != "":
            receivers.add(to)
        if frm != "":
            senders.add(frm)
        # simple contract interaction heuristic: if 'erc' in category or to/from is contract when known (we'll refine)
        # categories field exists: external/internal/erc20...
        cat = t.get("category")
        if isinstance(cat, list):
            # any erc* indicates token/contract interaction
            if any([c.startswith("erc") for c in cat if isinstance(c,str)]):
                contract_interactions += 1
        # timestamps: try metadata.blockTimestamp first
        meta = t.get("metadata", {}) or {}
        ts = meta.get("blockTimestamp") or meta.get("timestamp") or None
        ts_epoch = parse_timestamp(ts)
        if ts_epoch:
            timestamps_all.append(ts_epoch)
            if frm == addr_l:
                timestamps_sent.append(ts_epoch)
            if to == addr_l:
                timestamps_recv.append(ts_epoch)

    feat["alchemy_tx_count"] = len(transfers)
    feat["alchemy_sent_count"] = len(sent)
    feat["alchemy_recv_count"] = len(recv)
    feat["alchemy_unique_senders"] = len(senders)
    feat["alchemy_unique_receivers"] = len(receivers)
    feat["alchemy_contract_interactions"] = contract_interactions

    def avg_min_between(times):
        if len(times) < 2:
            return 0.0
        times = sorted(times)
        diffs = [(t2 - t1)/60.0 for t1,t2 in zip(times[:-1], times[1:])]
        return float(sum(diffs)/len(diffs)) if diffs else 0.0

    feat["alchemy_avg_min_between_sent"] = avg_min_between(timestamps_sent)
    feat["alchemy_avg_min_between_recv"] = avg_min_between(timestamps_recv)
    if timestamps_all:
        feat["alchemy_time_span_mins"] = (max(timestamps_all) - min(timestamps_all))/60.0
    else:
        feat["alchemy_time_span_mins"] = 0.0

    return feat

# ---------- main loop: load labels, iterate addresses, fetch/cache features ----------
print("Loading labels...")
if not os.path.exists(LABELS_FP):
    raise FileNotFoundError(f"{LABELS_FP} not found in working directory. Put the labels CSV here.")

labels_df = pd.read_csv(LABELS_FP, dtype=str)
if 'address' not in labels_df.columns:
    # attempt to rename first column to address
    labels_df = labels_df.rename(columns={labels_df.columns[0]:'address'})

addresses = labels_df['address'].astype(str).str.lower().unique().tolist()
print("Unique labeled addresses:", len(addresses))

cache = load_cache(CACHE_FP)
new_cache_entries = 0

# iterate addresses and fetch Alchemy data (with simple rate-limit handling)
for addr in tqdm(addresses, desc="Enrich addresses"):
    if addr in cache:
        continue
    # fetch transfers
    transfers = get_transfers_for_address(addr, max_count=MAX_TRANSFER_COUNT)
    # compute features
    feat = compute_features_from_transfers(addr, transfers)
    # get code to detect contract (best-effort)
    code = get_code(addr)
    if code is None:
        # network error / maybe rate-limited; set missing flag and continue but don't crash
        feat["alchemy_missing"] = 1
    else:
        # code == '0x' or '0x0' indicates EOA (externally owned account), else contract
        try:
            is_contract = 0 if (code in [None, "0x", "0x0", "0x00"]) else 1
        except:
            is_contract = 0
        feat["alchemy_is_contract"] = int(is_contract)
    # save raw minimal data in cache to avoid re-fetching
    cache[addr] = {"fetched_at": time.time(), "features": feat}
    new_cache_entries += 1
    # simple politeness: sleep a tiny bit to reduce rate-limit risk
    time.sleep(0.25)

# write cache back
save_cache(cache, CACHE_FP)
print(f"Cached {new_cache_entries} new addresses to {CACHE_FP} (total cache size: {len(cache)})")

# ---------- merge features back into labels_df and save ----------
print("Merging features into labels dataframe...")
feat_rows = []
for idx, row in labels_df.iterrows():
    a = str(row['address']).lower()
    if a in cache and cache[a].get("features") is not None:
        f = cache[a]["features"]
    else:
        f = {
            "alchemy_tx_count": 0,
            "alchemy_sent_count": 0,
            "alchemy_recv_count": 0,
            "alchemy_unique_senders": 0,
            "alchemy_unique_receivers": 0,
            "alchemy_avg_min_between_sent": 0.0,
            "alchemy_avg_min_between_recv": 0.0,
            "alchemy_time_span_mins": 0.0,
            "alchemy_contract_interactions": 0,
            "alchemy_is_contract": 0,
            "alchemy_missing": 1
        }
    feat_rows.append(f)

feat_df = pd.DataFrame(feat_rows)
merged = pd.concat([labels_df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)

# Save merged CSV
merged.to_csv(OUT_CSV, index=False)
print("Saved merged CSV to", OUT_CSV)

# Also save a numpy array of the new features for quick loading if you want
feature_cols = ["alchemy_tx_count","alchemy_sent_count","alchemy_recv_count",
                "alchemy_unique_senders","alchemy_unique_receivers",
                "alchemy_avg_min_between_sent","alchemy_avg_min_between_recv","alchemy_time_span_mins",
                "alchemy_contract_interactions","alchemy_is_contract","alchemy_missing"]
np.save("artifacts/alchemy_features.npy", merged[feature_cols].fillna(0).to_numpy())
print("Saved artifacts/alchemy_features.npy with shape", merged[feature_cols].shape)
print("Done — Alchemy enrichment finished. Tip: re-run this cell anytime to refresh the cache and update data.")

import pandas as pd
from collections import Counter
df = pd.read_csv("artifacts/labels_with_alchemy.csv")
cnts = Counter(df['final_label'].dropna().astype(str).values)
print(cnts)
# show classes with <=1 or <=4 counts
print("<=1:", [c for c,n in cnts.items() if n<=1])
print("<=4:", [c for c,n in cnts.items() if n<=4])

# Quick A/B: Kaggle-only vs Kaggle+Alchemy (XGBoost/RF)
!pip install -q xgboost --upgrade

import os, json, joblib, numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

ART = "artifacts"
os.makedirs(ART, exist_ok=True)
PLOTS = os.path.join(ART, "plots"); os.makedirs(PLOTS, exist_ok=True)

df = pd.read_csv(os.path.join(ART, "labels_with_alchemy.csv"))
df = df.dropna(subset=["final_label"]).reset_index(drop=True)

# define features
tab_cols = ['Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
            'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses']
alchemy_cols = [c for c in df.columns if c.startswith("alchemy_")]
print("Detected alchemy cols:", alchemy_cols)

# ensure columns exist
# ensure tabular columns exist and are numeric
for c in tab_cols:
    if c not in df.columns:
        df[c] = 0.0
    else:
        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)


# labels
le = LabelEncoder()
y = le.fit_transform(df['final_label'])
classes = le.classes_.tolist()
print("Classes:", classes)

# train/test (stratified)
X_base = df[tab_cols].fillna(0).values
X_aug  = df[tab_cols + alchemy_cols].fillna(0).values

Xb_tr, Xb_te, ytr, yte = train_test_split(X_base, y, test_size=0.2, stratify=y, random_state=42)
Xa_tr, Xa_te, _, _ = train_test_split(X_aug, y, test_size=0.2, stratify=y, random_state=42)

# choose model
def get_model():
    try:
        import xgboost as xgb
        m = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, n_jobs=4, random_state=42)
        print("Using XGBoost")
    except Exception:
        m = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)
        print("XGBoost not available. Using RandomForest")
    return m

results = {}
for name, Xtr, Xte in [("base", Xb_tr, Xb_te), ("aug", Xa_tr, Xa_te)]:
    m = get_model()
    m.fit(Xtr, ytr)
    preds = m.predict(Xte)
    acc = accuracy_score(yte, preds)
    f1 = f1_score(yte, preds, average='macro')
    print(f"\n{name} | acc={acc:.4f} | macro-F1={f1:.4f}")
    print(classification_report(yte, preds, target_names=classes, zero_division=0))
    results[name] = {"acc": float(acc), "f1_macro": float(f1)}
    joblib.dump(m, os.path.join(ART, f"ablation_{name}_model.joblib"))

# Save comparison and bar plot
with open(os.path.join(ART, "ablation_kaggle_vs_alchemy.json"), "w") as f:
    json.dump(results, f, indent=2)

plt.figure(figsize=(5,3))
names = list(results.keys()); f1s=[results[n]["f1_macro"] for n in names]
plt.bar(names, f1s)
plt.ylim(0,1); plt.title("Kaggle-only vs Kaggle+Alchemy Macro-F1")
for i,v in enumerate(f1s): plt.text(i, v+0.01, f"{v:.3f}", ha='center')
plt.savefig(os.path.join(PLOTS, "ablation_kaggle_vs_alchemy.png"), dpi=200)
plt.close()
print("Saved ablation_kaggle_vs_alchemy.png and models in artifacts/")

from collections import Counter
cnts = Counter(df['final_label'].astype(str).values)
# choose threshold, e.g. 2 or 5
THRESH = 5
valid = [lbl for lbl,c in cnts.items() if c >= THRESH]
print("Keeping classes:", valid)
mask = df['final_label'].astype(str).isin(valid)
df2 = df[mask].reset_index(drop=True)
# then rebuild X_base, X_aug, y from df2

# Robust A/B cell: handles missing tabular column names by fuzzy matching, falls back to alchemy-only if needed.
# Paste & run in Colab.

import os, json, joblib, numpy as np, pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

ART = "artifacts"
PLOTS = os.path.join(ART, "plots"); os.makedirs(PLOTS, exist_ok=True)

df = pd.read_csv(os.path.join(ART, "labels_with_alchemy.csv"))
print("Loaded df with shape:", df.shape)
print("\nColumns in dataframe:")
print(df.columns.tolist())

# Desired tabular columns (your original schema)
desired_tab_cols = [
    'Avg min between sent tnx',
    'Avg min between received tnx',
    'Time Diff between first and last (Mins)',
    'Sent tnx',
    'Received Tnx',
    'Unique Received From Addresses',
    'Unique Sent To Addresses'
]

# utility to normalize names (remove non-alnum, lower)
import re
def norm(s):
    return re.sub(r'[^0-9a-z]', '', str(s).lower())

cols = df.columns.tolist()
normed_map = {norm(c): c for c in cols}

# attempt to map desired_tab_cols to actual df columns
mapped = {}
missing = []
for c in desired_tab_cols:
    key = norm(c)
    if key in normed_map:
        mapped[c] = normed_map[key]
    else:
        # try partial match: find any column that contains key or key contains column part
        found = None
        for nc, orig in normed_map.items():
            if key in nc or nc in key:
                found = orig; break
        if found:
            mapped[c] = found
        else:
            missing.append(c)

print("\nColumn mapping results:")
for k,v in mapped.items():
    print(f"  desired: '{k}'  -> actual: '{v}'")
if missing:
    print("\nMissing desired columns (will fallback):")
    for m in missing:
        print("  ", m)

# Build final tab_cols list using mapped columns that exist
tab_cols_present = list(mapped.values())

# Build alchemy_cols excluding alchemy_notes
alchemy_cols = [c for c in df.columns if c.startswith("alchemy_") and c != "alchemy_notes"]
print("\nDetected alchemy cols:", alchemy_cols)

# If none of the tabular columns found, fallback to using only alchemy columns
if len(tab_cols_present) < 1:
    print("\nNo original tabular columns detected. Falling back to Alchemy-only features for A/B test.")
    use_tab = False
else:
    use_tab = True
    print("\nUsing tabular columns (mapped):", tab_cols_present)

# Filter out tiny classes
cnts = Counter(df['final_label'].dropna().astype(str).values)
print("\nClass counts:", cnts)
THRESH = 5
valid = [lbl for lbl,c in cnts.items() if c >= THRESH]
print("Keeping classes (count >= {}): {}".format(THRESH, valid))
df = df[df['final_label'].astype(str).isin(valid)].reset_index(drop=True)
print("After filtering shape:", df.shape)

# Build feature matrices
if use_tab:
    # ensure numeric
    for c in tab_cols_present:
        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)
    X_tab_all = df[tab_cols_present].values
else:
    X_tab_all = None

# alchemy numeric cleanup
for c in alchemy_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)
X_alchemy_all = df[alchemy_cols].values

# Build X_base and X_aug:
if use_tab:
    X_base = X_tab_all.copy()
    X_aug  = np.hstack([X_tab_all, X_alchemy_all]) if len(alchemy_cols)>0 else X_tab_all.copy()
    print("\nFeature dims: base =", X_base.shape, "aug =", X_aug.shape)
else:
    # no tab columns available: both base and aug are alchemy-only (degenerate A/B)
    X_base = X_alchemy_all.copy()
    X_aug = X_alchemy_all.copy()
    print("\nFeature dims: using alchemy-only features dims =", X_base.shape)

# Labels
le = LabelEncoder()
y = le.fit_transform(df['final_label'].astype(str).values)
classes = le.classes_.tolist()
print("\nClasses used:", classes)

# Train/test split with stratify (safe now since we filtered)
Xb_tr, Xb_te, ytr, yte = train_test_split(X_base, y, test_size=0.2, stratify=y, random_state=42)
Xa_tr, Xa_te, _, _ = train_test_split(X_aug, y, test_size=0.2, stratify=y, random_state=42)

# model factory with fallback
def get_model():
    try:
        import xgboost as xgb
        m = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, n_jobs=4, random_state=42)
        print("Using XGBoost")
    except Exception:
        m = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)
        print("XGBoost not available. Using RandomForest")
    return m

results = {}
for name, Xtr, Xte in [("base", Xb_tr, Xb_te), ("aug", Xa_tr, Xa_te)]:
    m = get_model()
    m.fit(Xtr, ytr)
    preds = m.predict(Xte)
    acc = accuracy_score(yte, preds)
    f1 = f1_score(yte, preds, average='macro')
    print(f"\n{name} | acc={acc:.4f} | macro-F1={f1:.4f}")
    print(classification_report(yte, preds, target_names=classes, zero_division=0))
    results[name] = {"acc": float(acc), "f1_macro": float(f1)}
    joblib.dump(m, os.path.join(ART, f"ablation_{name}_model.joblib"))

# Save comparison and bar plot
with open(os.path.join(ART, "ablation_kaggle_vs_alchemy.json"), "w") as f:
    json.dump(results, f, indent=2)

plt.figure(figsize=(5,3))
names = list(results.keys()); f1s=[results[n]["f1_macro"] for n in names]
plt.bar(names, f1s)
plt.ylim(0,1); plt.title("Kaggle-only vs Kaggle+Alchemy Macro-F1")
for i,v in enumerate(f1s): plt.text(i, v+0.01, f"{v:.3f}", ha='center')
plt.savefig(os.path.join(PLOTS, "ablation_kaggle_vs_alchemy.png"), dpi=200)
plt.close()
print("\nSaved ablation_kaggle_vs_alchemy.png and models in artifacts/")

# Full hybrid PyTorch retrain with Alchemy features + embeddings (checkpointing + plots)
# Paste into Colab and run. Assumes artifacts/labels_with_alchemy.csv exists.
# Recommended: run under caffeinate on your Mac if you want to keep the machine awake.

!pip install -q torch torchvision --upgrade

import os, json, time
import numpy as np
import pandas as pd
import joblib
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# ---------------- CONFIG ----------------
ART = "artifacts"
PLOT_DIR = os.path.join(ART, "plots"); os.makedirs(PLOT_DIR, exist_ok=True)
PT_DIR = os.path.join(ART, "pytorch"); os.makedirs(PT_DIR, exist_ok=True)
SAVED_SCALER = os.path.join(ART, "hybrid_with_alchemy_scaler.joblib")
METRICS_OUT = os.path.join(ART, "metrics_hybrid_with_alchemy.json")
CHECKPOINT_EVERY = 5
EPOCHS = 50
BATCH_SIZE = 64
LR = 1e-3
WEIGHT_DECAY = 1e-5
THRESH_MIN_CLASS = 5  # drop classes with fewer than this many examples
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)

# ---------------- Load merged dataset ----------------
dfp = os.path.join(ART, "labels_with_alchemy.csv")
if not os.path.exists(dfp):
    raise FileNotFoundError(f"{dfp} not found. Run Alchemy enrichment first.")

df = pd.read_csv(dfp)
print("Loaded df:", df.shape)

# list detected alchemy cols (exclude notes)
alchemy_cols = [c for c in df.columns if c.startswith("alchemy_") and c!="alchemy_notes"]
print("Alchemy cols:", alchemy_cols)

# attempt to find original tabular columns by fuzzy mapping (if present)
desired_tab_cols = [
    'Avg min between sent tnx','Avg min between received tnx','Time Diff between first and last (Mins)',
    'Sent tnx','Received Tnx','Unique Received From Addresses','Unique Sent To Addresses'
]
import re
def norm(s): return re.sub(r'[^0-9a-z]','', str(s).lower())
cols_map = {norm(c): c for c in df.columns.tolist()}
mapped_tab = []
for c in desired_tab_cols:
    k = norm(c)
    if k in cols_map:
        mapped_tab.append(cols_map[k])
# If no tabular cols mapped, we will proceed with alchemy-only
if len(mapped_tab) < 1:
    print("No original tabular cols detected; proceeding with Alchemy-only features.")
else:
    print("Using tabular columns (mapped):", mapped_tab)

# Final feature columns
if len(mapped_tab) >= 1:
    feature_cols = mapped_tab + alchemy_cols
else:
    feature_cols = alchemy_cols.copy()

# Filter tiny classes
cnts = Counter(df['final_label'].dropna().astype(str).values)
print("Class counts (before):", cnts)
valid_labels = [lbl for lbl,c in cnts.items() if c >= THRESH_MIN_CLASS]
print("Keeping labels (>= {} samples): {}".format(THRESH_MIN_CLASS, valid_labels))
df = df[df['final_label'].astype(str).isin(valid_labels)].reset_index(drop=True)
print("After filtering df shape:", df.shape)

# Build feature matrix
# ensure numeric
for c in feature_cols:
    df[c] = pd.to_numeric(df.get(c, 0), errors='coerce').fillna(0.0)
X_tab = df[feature_cols].values

# attempt load embeddings; require same rowcount to align
emb_path = os.path.join(ART, "structural_embeddings_svd.npy")
emb = None
if os.path.exists(emb_path):
    try:
        emb_full = np.load(emb_path)
        if emb_full.shape[0] == len(pd.read_csv(os.path.join(ART,"labels_final_patched_autolabeled.csv"))):
            # if original labels order aligned to embeddings, try to map via address
            # fallback: if emb rows == df rows (post-filter), use subset by mask
            # We'll try simple alignment by index only if lengths match dataset before filtering
            # Safe option: if embeddings length equals current df length, use directly
            if emb_full.shape[0] == len(df):
                emb = emb_full
                print("Using embeddings (direct match). shape:", emb.shape)
            else:
                # not safe to assume direct mapping; try node_map file
                node_map_fp = os.path.join(ART, "node_map.json")
                if os.path.exists(node_map_fp):
                    node_map = json.load(open(node_map_fp))
                    # build embedding matrix aligned to df rows if possible
                    emb_list = []
                    missing_emb_count = 0
                    for addr in df['address'].astype(str).str.lower().tolist():
                        idx = node_map.get(addr)
                        if idx is None:
                            emb_list.append(np.zeros((emb_full.shape[1],), dtype=float))
                            missing_emb_count += 1
                        else:
                            emb_list.append(emb_full[int(idx)])
                    emb = np.vstack(emb_list)
                    print(f"Built embeddings via node_map. missing_emb_count={missing_emb_count}")
                else:
                    print("Embeddings present but rowcounts don't match and node_map.json missing; skipping embeddings.")
        else:
            print("Embeddings file present but cannot align to dataset; skipping embeddings.")
    except Exception as e:
        print("Error loading embeddings:", e)
else:
    print("No embeddings file found; training on tabular+alchemy only.")

# Encode labels
le = LabelEncoder()
y = le.fit_transform(df['final_label'].astype(str).values)
classes = le.classes_.tolist()
print("Classes used:", classes)

# Train/test split (stratified)
if emb is not None:
    # ensure emb shape aligns with df
    X_full = np.hstack([X_tab, emb])
else:
    X_full = X_tab.copy()

X_tr, X_te, y_tr, y_te = train_test_split(X_full, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)
print("Train/test shapes:", X_tr.shape, X_te.shape)

# scale tabular+alchemy portion (first N cols)
n_tab_feats = X_tab.shape[1]
scaler = StandardScaler().fit(X_tr[:, :n_tab_feats])
# save scaler
joblib.dump(scaler, SAVED_SCALER)
# scale in-place
X_tr[:, :n_tab_feats] = scaler.transform(X_tr[:, :n_tab_feats])
X_te[:, :n_tab_feats] = scaler.transform(X_te[:, :n_tab_feats])

# Prepare PyTorch dataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

X_tr_t = torch.tensor(X_tr, dtype=torch.float32)
y_tr_t = torch.tensor(y_tr, dtype=torch.long)
train_ds = TensorDataset(X_tr_t, y_tr_t)
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)

# Model
class MLP(nn.Module):
    def __init__(self, in_dim, hidden=(256,128), num_classes=4, dropout=0.3):
        super().__init__()
        layers=[]
        prev=in_dim
        for h in hidden:
            layers.append(nn.Linear(prev,h)); layers.append(nn.ReLU()); layers.append(nn.Dropout(dropout))
            prev=h
        layers.append(nn.Linear(prev,num_classes))
        self.net = nn.Sequential(*layers)
    def forward(self,x):
        return self.net(x)

in_dim = X_tr.shape[1]; num_classes = len(classes)
model = MLP(in_dim=in_dim, hidden=(256,128), num_classes=num_classes).to(device)

# class weights (inverse frequency)
counts = np.bincount(y_tr)
counts = np.where(counts==0, 1, counts)
class_weights = (len(y_tr) / (len(counts) * counts)).astype(float)
weight_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)
print("Class weights:", class_weights.tolist())

criterion = nn.CrossEntropyLoss(weight=weight_tensor)
optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

# Training loop with checkpointing
best_f1 = -1.0
history = {"epochs": []}
for epoch in range(1, EPOCHS+1):
    model.train()
    running_loss = 0.0
    for xb, yb in train_loader:
        xb = xb.to(device); yb = yb.to(device)
        logits = model(xb)
        loss = criterion(logits, yb)
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        running_loss += loss.item() * xb.size(0)
    avg_loss = running_loss / len(train_loader.dataset)

    # validation
    model.eval()
    with torch.no_grad():
        Xv = torch.tensor(X_te, dtype=torch.float32).to(device)
        logits = model(Xv)
        preds = logits.argmax(dim=1).cpu().numpy()
    val_f1 = f1_score(y_te, preds, average='macro')
    val_acc = accuracy_score(y_te, preds)
    history["epochs"].append({"epoch": epoch, "loss": float(avg_loss), "val_acc": float(val_acc), "val_f1": float(val_f1)})
    print(f"Epoch {epoch}/{EPOCHS} loss={avg_loss:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f}")

    # checkpoint
    if epoch % CHECKPOINT_EVERY == 0:
        ckpt_fp = os.path.join(PT_DIR, f"hybrid_with_alchemy_epoch{epoch}.pth")
        torch.save({"epoch": epoch, "state_dict": model.state_dict(), "optimizer": optimizer.state_dict()}, ckpt_fp)
        print("Saved checkpoint:", ckpt_fp)

    # save best
    if val_f1 > best_f1:
        best_f1 = val_f1
        best_fp = os.path.join(PT_DIR, "hybrid_with_alchemy_best.pth")
        torch.save({"epoch": epoch, "state_dict": model.state_dict(), "classes": classes}, best_fp)
        print("Saved best model:", best_fp)

# Final evaluation on test
best = torch.load(os.path.join(PT_DIR, "hybrid_with_alchemy_best.pth"), map_location=device)
model.load_state_dict(best["state_dict"])
model.eval()
with torch.no_grad():
    logits = model(torch.tensor(X_te, dtype=torch.float32).to(device))
    preds = logits.argmax(dim=1).cpu().numpy()

acc = accuracy_score(y_te, preds)
f1m = f1_score(y_te, preds, average='macro')
prec, rec, f1s, sup = precision_recall_fscore_support(y_te, preds, labels=range(len(classes)), zero_division=0)
report = classification_report(y_te, preds, target_names=classes, zero_division=0)
print("Final test acc:", acc, "f1_macro:", f1m)
print(report)

# Save metrics JSON
metrics = {"acc": float(acc), "f1_macro": float(f1m), "per_class": {}}
for i,c in enumerate(classes):
    metrics["per_class"][c] = {"precision": float(prec[i]), "recall": float(rec[i]), "f1": float(f1s[i])}
with open(METRICS_OUT, "w") as f:
    json.dump(metrics, f, indent=2)
print("Saved metrics to", METRICS_OUT)

# Save confusion matrix plot
cm = confusion_matrix(y_te, preds, labels=range(len(classes)))
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix - Hybrid w/ Alchemy")
plt.tight_layout()
plt.savefig(os.path.join(PLOT_DIR, "confusion_hybrid_with_alchemy.png"), dpi=200)
plt.close()
print("Saved confusion plot")

# Per-class bar chart (precision/recall)
x = np.arange(len(classes))
width = 0.35
plt.figure(figsize=(7,4))
plt.bar(x - width/2, prec, width, label="Precision")
plt.bar(x + width/2, rec, width, label="Recall")
plt.xticks(x, classes, rotation=25)
plt.ylim(0,1)
plt.legend()
plt.title("Per-class Precision & Recall - Hybrid w/ Alchemy")
plt.tight_layout()
plt.savefig(os.path.join(PLOT_DIR, "perclass_pr_hybrid_with_alchemy.png"), dpi=200)
plt.close()
print("Saved per-class PR plot")

# Save history
with open(os.path.join(ART, "pytorch", "hybrid_with_alchemy_history.json"), "w") as f:
    json.dump(history, f, indent=2)

# Export lightweight sklearn wrapper: fit a logistic regression on learned representations (optional)
try:
    from sklearn.linear_model import LogisticRegression
    # get hidden representations for train and test by forwarding through model up to penultimate layer
    def get_reps(X_np):
        # extract features from model up to last linear (we assume architecture structure from MLP)
        model.eval()
        with torch.no_grad():
            x_t = torch.tensor(X_np, dtype=torch.float32).to(device)
            # forward through all layers except final Linear
            # hack: pass through sequential until last layer index-1
            feat = x_t
            for layer in list(model.net.children())[:-1]:
                feat = layer(feat)
            return feat.cpu().numpy()
    reps_tr = get_reps(X_tr); reps_te = get_reps(X_te)
    clf = LogisticRegression(multi_class="multinomial", solver="lbfgs", max_iter=500)
    clf.fit(reps_tr, y_tr)
    joblib.dump(clf, os.path.join(ART, "hybrid_with_alchemy_rep_logreg.joblib"))
    print("Saved sklearn wrapper on learned reps")
except Exception as e:
    print("Could not export sklearn wrapper:", e)

print("TRAINING COMPLETE. Artifacts saved under 'artifacts/'")

